<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2019论文阅读笔记&lt;01&gt;</title>
      <link href="/2019/09/18/article/2019/01/"/>
      <url>/2019/09/18/article/2019/01/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-论文阅读笔记-lt-01-gt"><a href="#2019-论文阅读笔记-lt-01-gt" class="headerlink" title="2019 论文阅读笔记 &lt;01&gt;"></a>2019 论文阅读笔记 &lt;01&gt;</h1><h2 id="【基本信息】"><a href="#【基本信息】" class="headerlink" title="【基本信息】"></a>【基本信息】</h2><ul><li><strong>论文题目：</strong> <em><a href="https://ieeexplore.ieee.org/document/7372153" target="_blank" rel="noopener">Entropy-Based Term Weighting Schemes for Text Categorization in VSM</a></em></li><li><strong>作者：</strong>Tao Wang ; Yi Cai ; Ho-fung Leung ; Zhiwei Cai ; Huaqing Min</li><li><strong>出版：</strong> <a href="https://ieeexplore.ieee.org/xpl/conhome/7372093/proceeding" target="_blank" rel="noopener">2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)</a></li><li><strong>日期：</strong>9-11 Nov. 2015 / 07 Jan 2016</li><li><strong>标签：</strong>TC、VSM</li><li><strong>简介：</strong>此论文分析了用于文本分类任务的一些权重法（如无监督学习中的$tf,tf\cdot idf,BM25$ 与监督学习中的$rf,iqf\cdot qf\cdot icf,tf\cdot gr$等），指出这些已有模式在文本分类中存在的问题，并提出了两种新的基于熵的权重法（$tf\cdot dc$ 和 $tf\cdot bdc$），提升了术语的辨别力与文本分类任务的完成。</li></ul><h2 id="【词汇术语】"><a href="#【词汇术语】" class="headerlink" title="【词汇术语】"></a>【词汇术语】</h2><h3 id="专业词汇-术语"><a href="#专业词汇-术语" class="headerlink" title="专业词汇/术语"></a>专业词汇/术语</h3><ul><li><strong>TC</strong> （Text Categorization）：文本分类</li><li><strong>VSM</strong> （Vector Space Model）：向量空间模型</li><li><strong>IR</strong> （Information Retrieval）：信息检索</li><li><strong>unsupervised</strong> &amp; <strong>supervised</strong>：无监督、监督</li><li><strong>Contingency Table</strong> 情形分析表<ul><li><strong>positive category （PC）</strong> ：正类</li><li><strong>negative category （NC）</strong>：负类</li></ul></li></ul><h3 id="语言词汇"><a href="#语言词汇" class="headerlink" title="语言词汇"></a>语言词汇</h3><ul><li><strong>state-of-the-art</strong>：当前最好的</li></ul><h3 id="模型-模式-算法"><a href="#模型-模式-算法" class="headerlink" title="模型/模式/算法"></a>模型/模式/算法</h3><ul><li><strong>KNN</strong> （k-Nearest Neighbor）：邻近算法</li><li><strong>SVM</strong> （Support Vector Machine）：支持向量机</li><li>$tf$：<strong>Term Frequency</strong> 词频<ul><li>$词频(TF)=某个词在文章中的出现次数$</li><li>$词频(TF)=\frac{某个词在文章中的出现次数}{文章的总词数}$</li><li>$词频(TF)=\frac{某个词在文章中的出现次数}{该文出现次数最多的词的出现次数}$</li><li>Variants: $\log(tf),\log(tf+1),log(tf)+1,…$</li></ul></li><li>$idf$: <strong>Inverse document frequency</strong> 逆文本频率指数<ul><li>$逆文档频率(IDF)=\log(\frac{语料库的文档总数}{包含该词的文档数+1})$</li><li>$df$: <strong>document frequency</strong></li><li>包含词条的文档越小，$idf$越大</li></ul></li><li>$dc$: <strong>distributional concentration</strong><ul><li>$dc(t)=1-\frac{H(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{f(t,c_i)}{f(t)}\log{\frac{f(t,c_i)}{f(t)}}}{\log{|C|}}$</li></ul></li><li>$rf$: <strong>relevance frequency</strong>, $rf=\frac{a}{c}$</li><li>$cf$: <strong>category frequency</strong>, 出现的类别越少，值越大</li><li>$bdc$: <strong>balanced distributional concentration</strong><ul><li>$bdc(t)=1-\frac{BH(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{p(t|c_i)}{\sum^{|C|}_{i=1}p(t|c_i)}\log{\frac{p(t|c_i)}{sum^{|C|}_{i=1}p(t|c_i)}}}{\log{|C|}}$</li></ul></li></ul><h2 id="【论文笔记】"><a href="#【论文笔记】" class="headerlink" title="【论文笔记】"></a>【论文笔记】</h2><h3 id="「已有方法-—-无监督学习方法」"><a href="#「已有方法-—-无监督学习方法」" class="headerlink" title="「已有方法 —- 无监督学习方法」"></a>「已有方法 —- 无监督学习方法」</h3><h4 id="无监督学习（unsupervised）"><a href="#无监督学习（unsupervised）" class="headerlink" title="无监督学习（unsupervised）"></a>无监督学习（unsupervised）</h4><ul><li><strong>含义：</strong>无先验知识（无标签）的学习</li><li><strong>常见模式：</strong>$tf,tf\cdot idf,BM25 $</li></ul><h4 id="缺点问题"><a href="#缺点问题" class="headerlink" title="缺点问题"></a>缺点问题</h4><ul><li><strong>【由于】</strong>关注于词出现的次数，而忽略了训练<em>文档（documents）<em>的</em>类别标签（category labels）</em></li><li><strong>【导致】</strong><ul><li><em>词语（term）</em>能区别文档的差异，但不能区别类别的差异</li><li>在文本分类任务中，不足以衡量词语对文档类别的<em>辨别能力（discriminating power）</em></li></ul></li></ul><h3 id="「已有方法-—-监督学习方法」"><a href="#「已有方法-—-监督学习方法」" class="headerlink" title="「已有方法 —- 监督学习方法」"></a>「已有方法 —- 监督学习方法」</h3><h4 id="监督学习（supervised）"><a href="#监督学习（supervised）" class="headerlink" title="监督学习（supervised）"></a>监督学习（supervised）</h4><ul><li><strong>含义：</strong>利用已知类别的样本进行学习</li><li><strong>常见模式：</strong>$rf,iqf\cdot qf\cdot icf,tf\cdot gr$</li></ul><h4 id="缺点问题-1"><a href="#缺点问题-1" class="headerlink" title="缺点问题"></a>缺点问题</h4><ul><li><strong>缘由：</strong>大部分监督学习用到了情形分析表中的正类（PC）和负类（NC）</li><li><strong>【由于】</strong>在<em>多类别的情况（multi-class case）</em>中，正类只有一个类，而负类是多个类的集合</li><li><strong>【导致】</strong>负类中产生了<em>信息损失（information loss）</em></li></ul><h4 id="已有优化仍然存在的问题"><a href="#已有优化仍然存在的问题" class="headerlink" title="已有优化仍然存在的问题"></a>已有优化仍然存在的问题</h4><ul><li><em>正类和负类的分离问题 （PC/NC-split based schemes）</em>仍然存在，使得无法有效区别类别。</li><li><em>一种基于统计置信区间的模式（a scheme based on statistical confidence intervals）</em><ul><li>过于复杂难以实现</li></ul></li></ul><h3 id="「基于熵的权重法」"><a href="#「基于熵的权重法」" class="headerlink" title="「基于熵的权重法」"></a>「基于熵的权重法」</h3><h4 id="基于熵的权重法（entropy-based-term-weighting-schemes）"><a href="#基于熵的权重法（entropy-based-term-weighting-schemes）" class="headerlink" title="基于熵的权重法（entropy- based term weighting schemes）"></a>基于熵的权重法（entropy- based term weighting schemes）</h4><ul><li><strong>观点</strong><ul><li>利用负类中的具体类别帮助提升辨别力</li><li><em>更高浓度（higher concentration）</em>的词语具有更强的辨别能力</li><li>在类别层面有更高浓度分布的词语的熵更小</li></ul></li><li>熵值越小，辨别能力越强</li></ul><h4 id="模式1-Distribution-Concentration"><a href="#模式1-Distribution-Concentration" class="headerlink" title="模式1: Distribution Concentration"></a>模式1: Distribution Concentration</h4><ul><li><strong>模式</strong>：$dc$, distribution concentration</li><li><strong>计算公式：</strong> $dc(t)=1-\frac{H(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{f(t,c_i)}{f(t)}\log{\frac{f(t,c_i)}{f(t)}}}{\log{|C|}}$<ul><li>$|C|$: 类别数量</li><li>$f(t,c_i)$: 表示词语 $t$ 在类别 $c_i$ 中出现的频率，这里使用 $df(t,c_i)$</li><li>$f(t)$: 词语 $t$ 在所有类别出现的频数和</li><li>$H(t)$: 词语 $t$ 在语料库中类别的熵，$H(t)\in [0,\log|C|]$</li></ul></li><li><strong>特征优势</strong><ul><li>词语的权重基于在类别中词语的全局分布，而不是依赖于已有的类别正类。</li><li>因此，不需要已有的正类标签就可以进行分类。</li></ul></li><li><strong>缺陷</strong><ul><li>缺少类别的优先级信息（不同种类文档的长度不同，会导致熵的计算产生偏差）<ul><li>$bdc$模式解决此问题</li></ul></li></ul></li></ul><h4 id="模式2-Balanced-Distributional-Concentration"><a href="#模式2-Balanced-Distributional-Concentration" class="headerlink" title="模式2: Balanced Distributional Concentration"></a>模式2: Balanced Distributional Concentration</h4><ul><li><strong>模式</strong>：$bdc$, balanced distributional concentration</li><li><strong>计算公式：</strong> $bdc(t)=1-\frac{BH(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{p(t|c_i)}{\sum^{|C|}_{i=1}p(t|c_i)}\log{\frac{p(t|c_i)}{sum^{|C|}_{i=1}p(t|c_i)}}}{\log{|C|}}$</li><li><strong>特征优势</strong><ul><li>解决了 $dc$ 模式存在的问题</li></ul></li></ul><h3 id="「实验结果」"><a href="#「实验结果」" class="headerlink" title="「实验结果」"></a>「实验结果」</h3><ul><li>此处对于实验的数据、内容、结论不做详细描述，有兴趣可以自行查看论文。</li></ul><h2 id="【小记总结】"><a href="#【小记总结】" class="headerlink" title="【小记总结】"></a>【小记总结】</h2><ul><li>2019论文博客整理的第一篇，希望能坚持，不足之处还请谅解。许多的英文词汇转为中文总有些怪异的感觉，就保留在中文词后面了。</li><li>此论文指出了过往文本分类任务中一些模式（scheme, 这个词翻译成模式也觉得读起来不太顺口，汗）的缺陷，无法很好地利用标签信息、区别类别差异，因而引入了熵的概念。一种角度来说，也是更充分地利用“压榨”已有信息（无监督学习模式忽略了类别标签，PC/NC的监督学习模式则忽略了NC中的类别差异），挖掘、压榨、充分利用全部信息，促进更好的分类。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
            <tag> 2019 </tag>
            
            <tag> TC </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
