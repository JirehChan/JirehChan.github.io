<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Data概念篇DM（一）数据挖掘基础概念</title>
      <link href="/data-concept-dm-01/"/>
      <url>/data-concept-dm-01/</url>
      
        <content type="html"><![CDATA[<h1 id="Data概念篇DM（一）数据挖掘基础概念"><a href="#Data概念篇DM（一）数据挖掘基础概念" class="headerlink" title="Data概念篇DM（一）数据挖掘基础概念"></a>Data概念篇DM（一）数据挖掘基础概念</h1><ul><li>参《数据挖掘导论（完整版）》第一章</li></ul><h2 id="什么是数据挖掘？"><a href="#什么是数据挖掘？" class="headerlink" title="什么是数据挖掘？"></a>什么是数据挖掘？</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li>数据挖掘是一种<strong>技术</strong>，将<strong>传统的数据分析方法</strong>与<strong>处理大量数据的复杂算法</strong>相结合。</li><li>数据挖掘是在大型数据存储库中，自动地发现有用信息的过程。数据挖掘技术用来探查大型数据库，发现先前未知的有用模式，还可以预测未来观测结果。</li></ul><h3 id="数据挖掘与知识发现"><a href="#数据挖掘与知识发现" class="headerlink" title="数据挖掘与知识发现"></a>数据挖掘与知识发现</h3><ul><li><p>数据挖掘是<strong>数据库中知识发现</strong>（knowledge discovery in database, KDD）不可缺少的一部分，KDD是将未加工的数据转换为有用信息的整个过程，如下图所示，</p><p><img src="./01-01.png" alt="01-01 KDD"></p></li><li><p>各个步骤：</p><ul><li><strong>输入数据：</strong>可以以各种形式存储，可以驻留在集中的数据存储库或分布在多个站点上。</li><li><strong>数据预处理</strong>（preprocessing）：将未加工的输入数据转换成适合分析的形式。<ul><li>包含<strong>融合</strong>来自多个数据源的数据、<strong>清洗</strong>数据以消除<strong>噪声</strong>和<strong>重复</strong>的观测值、<strong>选择</strong>与当前数据挖掘任务相关的记录和特征。</li><li>特征选择、维规约、规范化、选择数据子集</li></ul></li><li><strong>后处理：</strong>将有效的和有用的结果集成到决策支持系统中。<ul><li>可视化；使用统计度量或假设检验，删除虚假的数据挖掘结果。</li></ul></li></ul></li></ul><h2 id="数据问题需要的问题"><a href="#数据问题需要的问题" class="headerlink" title="数据问题需要的问题"></a>数据问题需要的问题</h2><ul><li><strong>可伸缩</strong>（scalable）<ul><li>使用特殊的搜索策略处理指数级搜索问题、实现新的数据结构、非内存算法、使用抽样技术或开发并行和分布算法</li></ul></li><li><strong>高维度</strong></li><li><strong>异种数据和复杂数据</strong></li><li><strong>数据的所有权与分布</strong></li><li><strong>非传统的分析</strong><ul><li>传统的统计方法基于<strong>假设-验证模式</strong></li></ul></li></ul><h2 id="数据挖掘的起源"><a href="#数据挖掘的起源" class="headerlink" title="数据挖掘的起源"></a>数据挖掘的起源</h2><ul><li><strong>统计学</strong>的抽样、估计和假设检验</li><li><strong>人工智能</strong>、<strong>模式识别</strong>和<strong>机器学习</strong>的搜索算法、建模技术和学习理论</li><li>其它领域，包括最优化、进化计算、信息论、信号处理、可视化和信息检索</li><li>其它方面，数据库系统、高性能（并行）计算、分布式技术</li></ul><h2 id="数据挖掘任务"><a href="#数据挖掘任务" class="headerlink" title="数据挖掘任务"></a>数据挖掘任务</h2><ul><li>两大类：<ul><li><strong>预测任务</strong>：根据其它属性的值（<strong>说明变量</strong>或<strong>自变量</strong>），预测特定属性的值（<strong>目标变量</strong>或<strong>因变量</strong>）</li><li><strong>描述任务</strong>：导出概括数据中潜在练习的模式（相关、趋势、聚类、轨迹和异常）</li></ul></li><li><strong>预测建模</strong>（predictive modeling）：以说明变量函数的方式为目标变量建立模型<ul><li><strong>分类</strong>（classification）：预测离散的目标变量</li><li><strong>回归</strong>（regression）：预测连续的目标变量</li></ul></li><li><strong>关联分析</strong>（association analysis）：发现描述数据中强关联特征的模式<ul><li>应用：找出具有相关功能的基因组、识别用户、理解元素之间的联系</li></ul></li><li><strong>聚类分析</strong>（cluster analysis）：发现紧密相关的预测值组群<ul><li>应用：对相关的顾客分组、找出显著影响地球气候的海洋区域、压缩数据</li></ul></li><li><strong>异常检测</strong>（anomaly detection）：识别特征显著不同于其他数据的观测值<ul><li><strong>异常点</strong>（anomaly）或<strong>离群点</strong>（outlier）</li><li>好的异常检测器：高检测率、低误报率</li><li>应用：检测欺诈、网络攻击、疾病的不寻常模式、生物系统扰乱等</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Data </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data总序篇：介绍与目录</title>
      <link href="/data-preface/"/>
      <url>/data-preface/</url>
      
        <content type="html"><![CDATA[<h1 id="Data总序篇：介绍与目录"><a href="#Data总序篇：介绍与目录" class="headerlink" title="Data总序篇：介绍与目录"></a>Data总序篇：介绍与目录</h1><h2 id="系列介绍"><a href="#系列介绍" class="headerlink" title="系列介绍"></a>系列介绍</h2><h3 id="缘由"><a href="#缘由" class="headerlink" title="缘由"></a>缘由</h3><ul><li>由于近期计划阅读、学习几本经典书籍，分别与数据挖掘、统计学方法与机器学习相关，所以计划编写一个系列的笔记。</li><li>由于这些内容存在着极多交叉相通的部分，所以希望能将其整合在一个系列之中，避免重复造轮子。</li><li>但另一方面，由于这些内容属于不同的主题，所以架构、展开方式存在着很大的差异，所以此系列的编排也存在着很大的挑战，希望能做好。最终取名为Data，是由于这些内容都与数据相关，所以就简单粗暴地命名了。</li></ul><h3 id="主要书目"><a href="#主要书目" class="headerlink" title="主要书目"></a>主要书目</h3><ul><li>整个系列的笔记，主要对应以下四本书：</li></ul><ol><li>《<a href="https://book.douban.com/subject/5377669/" target="_blank" rel="noopener">数据挖掘导论</a>》</li><li>《<a href="https://book.douban.com/subject/33437381/" target="_blank" rel="noopener">统计学习方法（第2版）</a>》</li><li>《<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">机器学习</a>》</li><li>《<a href="https://book.douban.com/subject/10750155/" target="_blank" rel="noopener">数学之美</a>》</li></ol><ul><li>这几本书在豆瓣上的评分都在8分以上，而且都作为领域中的经典书籍，希望能尽早啃完了（不知道能多久了，希望不会断片了…）</li></ul><h3 id="编排思路"><a href="#编排思路" class="headerlink" title="编排思路"></a>编排思路</h3><ul><li>本系列计划分为<strong>总序篇</strong>、<strong>概念篇</strong>、<strong>方法篇</strong>和<strong>补充篇</strong>四个不同篇章主题。</li><li><strong>总序篇</strong>：此文，包含系列的介绍和全部目录。</li><li><strong>概念篇</strong>：包含一些概念的内容，如数据挖掘、机器学习、数据等基本概念的介绍，也包含如分类问题、聚类问题、监督学习、强化学习等概念的介绍。</li><li><strong>方法篇</strong>：介绍具体的模型和算法，如kNN模型、决策树模型、神经网络模型等内容。</li><li><strong>补充篇</strong>：一些补充内容，如历史、发展、数学基础等内容。</li></ul><h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><ul><li>持续更新，当前很贫穷。</li></ul><h3 id="总序篇"><a href="#总序篇" class="headerlink" title="总序篇"></a>总序篇</h3><table><thead><tr><th>题目</th><th>简介</th></tr></thead><tbody><tr><td><a href>Data总序篇：介绍与目录</a></td><td>关于整个Data系列的介绍以及目录信息</td></tr></tbody></table><h3 id="概念篇"><a href="#概念篇" class="headerlink" title="概念篇"></a>概念篇</h3><table><thead><tr><th>题目</th><th>简介</th></tr></thead><tbody><tr><td><a href>Data概念篇DM（一）数据挖掘基础概念</a></td><td>数据挖掘的定义、问题、任务、应用</td></tr></tbody></table><h3 id="方法篇"><a href="#方法篇" class="headerlink" title="方法篇"></a>方法篇</h3><table><thead><tr><th>题目</th><th>简介</th></tr></thead><tbody><tr><td>暂无</td><td>暂无</td></tr></tbody></table><h3 id="补充篇"><a href="#补充篇" class="headerlink" title="补充篇"></a>补充篇</h3><table><thead><tr><th>题目</th><th>简介</th></tr></thead><tbody><tr><td>暂无</td><td>暂无</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data </tag>
            
            <tag> Preface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenGL学习笔记（二）渲染管线、FreeGLUT&amp;GLEW、简单示例</title>
      <link href="/opengl-02/"/>
      <url>/opengl-02/</url>
      
        <content type="html"><![CDATA[<h1 id="OpenGL学习笔记（二）渲染管线、FreeGLUT-amp-GLEW、简单示例"><a href="#OpenGL学习笔记（二）渲染管线、FreeGLUT-amp-GLEW、简单示例" class="headerlink" title="OpenGL学习笔记（二）渲染管线、FreeGLUT&amp;GLEW、简单示例"></a>OpenGL学习笔记（二）渲染管线、FreeGLUT&amp;GLEW、简单示例</h1><h2 id="1-渲染管线"><a href="#1-渲染管线" class="headerlink" title="1. 渲染管线"></a>1. 渲染管线</h2><ul><li>主要整理自<a href="https://learnopengl-cn.github.io/01%20Getting%20started/04%20Hello%20Triangle/" title="LearnOpenGL CN - 你好，三角形" target="_blank" rel="noopener">[1]</a></li></ul><h3 id="图形渲染管线（Graphics-Pipeline）"><a href="#图形渲染管线（Graphics-Pipeline）" class="headerlink" title="图形渲染管线（Graphics Pipeline）"></a>图形渲染管线（Graphics Pipeline）</h3><ul><li><p>OpenGL是一个3D图形库，即其中的所有事物都存储在3D空间中，而屏幕和窗口中则是显示为2D像素，<strong>图形渲染管线</strong>的作用就是实现2D坐标和3D坐标之间的互相转换。有以下两个部分：</p><ul><li>将3D坐标转换为2D坐标；</li><li>将2D坐标转变为实际有色的像素。</li></ul></li><li><p>图形渲染管线的各个阶段的展示图如下：</p><p><img src="./01-01.png" alt="01-01 图形渲染管线"></p></li><li><p><strong>着色器</strong>（Shader）是在渲染管线中，每个阶段上运行处理数据的小程序。OpenGL着色器是用<strong>OpenGL着色器语言</strong>（OpenGL Shading Language, GLSL）写成的。</p></li><li><p>接下来将逐一介绍渲染管线的各个阶段。</p></li></ul><h3 id="顶点数据（Vertex-Data）"><a href="#顶点数据（Vertex-Data）" class="headerlink" title="顶点数据（Vertex Data）"></a>顶点数据（Vertex Data）</h3><ul><li><p>顶点数据作为图形渲染管线的输入，是一系列<strong>顶点</strong>（Vertex）的集合。</p></li><li><p>顶点是一个3D坐标数据的集合，用<strong>顶点属性</strong>（Vertex Attribute）表示。</p></li><li><p>OpenGL中指定的坐标都是3D坐标，但只有在<strong>标准化设备坐标</strong>（Normalized Device Corrdinates NDC，即三个轴坐标都在-1.0到1.0的范围内）范围内的坐标才能呈现在屏幕上。</p><ul><li>标准化设备坐标之后会转化为<strong>屏幕空间坐标</strong>（Screen-space Coordinates），通过<em>glViewport</em>函数提供的数据，进行<strong>视口变换</strong>（Viewport Transform）完成。</li></ul></li><li><p>顶点数据定义后被传入顶点着色器中，会在GPU上创建内存用于存储顶点数据。通过<strong>顶点缓冲对象</strong>（Vertex Buffer Objects VBO）管理此内存，它会在GPU内存中存储大量顶点，然后批量发给显卡。</p></li><li><p><strong>顶点数组对象</strong>（Vertex Array Object, VAO）</p><ul><li>任何随后的顶点属性调用都会储存在这个VAO中。这样的好处就是，当配置顶点属性指针时，你只需要将那些调用执行一次，之后再绘制物体的时候只需要绑定相应的VAO就行了。这使在不同顶点数据和属性配置之间切换变得非常简单，只需要绑定不同的VAO就行了。</li></ul><p><img src="./01-02.png" alt="01-02 VAO与VBO"></p></li></ul><h3 id="顶点着色器（Vertex-Shader）"><a href="#顶点着色器（Vertex-Shader）" class="headerlink" title="顶点着色器（Vertex Shader）"></a>顶点着色器（Vertex Shader）</h3><ul><li><strong>顶点着色器</strong>是图形渲染管线的第一个部分，把一个单独的顶点作为输入。主要作用：<ul><li>把3D坐标转换为另一种3D坐标；</li><li>对顶点属性进行一些简单的处理。</li></ul></li></ul><h3 id="图元装配（Primitive-Assembly）"><a href="#图元装配（Primitive-Assembly）" class="headerlink" title="图元装配（Primitive Assembly）"></a>图元装配（Primitive Assembly）</h3><ul><li>将顶点着色器输出的所有顶点作为输入，将所有的点装配成制定图元的形状。</li></ul><h3 id="几何着色器（Geometry-Shader）"><a href="#几何着色器（Geometry-Shader）" class="headerlink" title="几何着色器（Geometry Shader）"></a>几何着色器（Geometry Shader）</h3><ul><li><strong>几何着色器</strong>把图元形式的一系列顶点的集合作为输入，可以通过产生新顶点构造出新的图元来生成其它形状。</li></ul><h3 id="光栅化（Rasterization）"><a href="#光栅化（Rasterization）" class="headerlink" title="光栅化（Rasterization）"></a>光栅化（Rasterization）</h3><ul><li><strong>光栅化</strong>把图元映射成对应的像素，生成片段（Fragment）。在<strong>片段着色器</strong>运行之前，会执行<strong>剪裁</strong>（Clipping）来提升执行效率。</li></ul><h3 id="片段着色器（Fragment-Shader）"><a href="#片段着色器（Fragment-Shader）" class="headerlink" title="片段着色器（Fragment Shader）"></a>片段着色器（Fragment Shader）</h3><ul><li><strong>片段着色器</strong>的主要目的是计算一个像素的最终颜色，也包含3D场景的数据（如光照、阴影、光的颜色等）。</li></ul><h3 id="Alpha测试和混合（Blending）"><a href="#Alpha测试和混合（Blending）" class="headerlink" title="Alpha测试和混合（Blending）"></a>Alpha测试和混合（Blending）</h3><ul><li>此阶段检测片段对应的深度，来判断这个像素对应其它物体的相对位置。也会检查alpha值来对物体进行混合。</li></ul><h2 id="2-FreeGLUT库-amp-GLEW库-配置"><a href="#2-FreeGLUT库-amp-GLEW库-配置" class="headerlink" title="2. FreeGLUT库 &amp; GLEW库 配置"></a>2. FreeGLUT库 &amp; GLEW库 配置</h2><ul><li>以下配置针对Mac上的Xcode配置，如是其他平台其它软件请自行百度，参考<a href="https://www.cnblogs.com/fanghao/p/7559768.html" title="Mac使用Xcode配置openGL -- 潇雨危栏" target="_blank" rel="noopener">[2]</a><a href="https://www.cnblogs.com/leojason/p/9619193.html" title="OpenGL学习之旅01—Xcode+OpenGL环境配置 -- LeoJason" target="_blank" rel="noopener">[3]</a>。（安装主要也是因为课程要求，汗）</li></ul><h3 id="库简介"><a href="#库简介" class="headerlink" title="库简介"></a>库简介</h3><ul><li><strong>FreeGLUT库</strong><ul><li>由于GLUT项目已被废弃（不再维护，无法修改），FreeGLUT是GLUT的一个完全开源替代库。</li></ul></li><li><strong>GLEW库</strong><ul><li>OpenGL扩展库，用于帮助C/C++开发者初始化扩展（OpenGL扩展功能）并书写可移植的应用程序。</li></ul></li></ul><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ol><li><p><strong>安装homebrew</strong></p><p>​    由于之前已经安装过了homebrew，是否当时使用的是以下命令已经不太清楚了，如果有问题 ，可以在网上寻找。</p><pre class=" language-shell"><code class="language-shell">$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</code></pre></li><li><p><strong>安装FreeGLUT</strong></p><pre class=" language-shell"><code class="language-shell">$ brew install freeglut</code></pre><p>​    安装中可能会提示需要先安装XQuartz，此时同样照着提示来安装，完成后再次brew即可。</p></li><li><p><strong>安装GLEW</strong></p><pre class=" language-shell"><code class="language-shell">$ brew install glew</code></pre><p>​    安装中可能会由于没有读写目录的权限的提示，此时只要按照终端的提示操作之后就可以了。</p></li><li><p><strong>安装后的库</strong></p><p>brew安装的目录在/usr/local/Cellar下，之后在Xcode中的配置会用到。</p></li></ol><h3 id="Xcode中的配置"><a href="#Xcode中的配置" class="headerlink" title="Xcode中的配置"></a>Xcode中的配置</h3><ol><li><p>打开Xcode，新建一个Command Line Tool项目。</p></li><li><p>在Build Settings中的Search Paths加入头文件、库文件搜索路径。</p><p><img src="./02-01.png" alt="02-01 加入搜索路径"></p></li><li><p>在Build Phases中Link Binary With Libraries中加入库</p><p>​    不知为何上步完成后仍是无法搜索到，只能手动添加文件了，另外不要忘了基本库OpenGL.framework哦。</p><p><img src="./02-02.png" alt="02-02 加入库"></p></li></ol><h2 id="3-简单示例"><a href="#3-简单示例" class="headerlink" title="3. 简单示例"></a>3. 简单示例</h2><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><ul><li><p>代码来自计算机图形学课程的讲义，绘制一个简单的三角形。</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;GL/glew.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;GL/glut.h></span></span><span class="token keyword">void</span> <span class="token function">doMyInit</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">glClearColor</span><span class="token punctuation">(</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// Set the clear color to black</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">mydisplay</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">glClear</span><span class="token punctuation">(</span> GL_COLOR_BUFFER_BIT<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// Clear the frame buffer</span>    <span class="token function">glColor3f</span><span class="token punctuation">(</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// Set current color to green</span>    <span class="token function">glBegin</span><span class="token punctuation">(</span> GL_TRIANGLES<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// Draw the triangle</span>        <span class="token function">glVertex2f</span><span class="token punctuation">(</span> <span class="token operator">-</span><span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">glVertex2f</span><span class="token punctuation">(</span> <span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">glVertex2f</span><span class="token punctuation">(</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glEnd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glFlush</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// Force to display the new drawings immediately</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span> <span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span><span class="token operator">*</span> argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// initialize</span>    <span class="token function">glutInit</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>argc<span class="token punctuation">,</span>argv<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitDisplayMode</span><span class="token punctuation">(</span>GLUT_RGB <span class="token operator">|</span> GLUT_SINGLE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitWindowSize</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">,</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitWindowPosition</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutCreateWindow</span><span class="token punctuation">(</span><span class="token string">"Simple"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glewInit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">doMyInit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//define callback functions</span>    <span class="token function">glutDisplayFunc</span><span class="token punctuation">(</span>mydisplay<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutMainLoop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//main event loop</span><span class="token punctuation">}</span></code></pre></li></ul><h3 id="然而。。。"><a href="#然而。。。" class="headerlink" title="然而。。。"></a>然而。。。</h3><ul><li><p>结果是无法成功跑出结果，显示如下：</p><img src="./03-01.png" alt="03-01 失败截图" style="zoom:50%;"></li><li><p>尝试多次后，放弃了，似乎是因为FreeGLUT无法在Mac上编译shader，只能重新改回原生的GLUT.framework了。</p></li></ul><h3 id="修改后"><a href="#修改后" class="headerlink" title="修改后"></a>修改后</h3><ul><li><strong>代码部分</strong><ul><li>将 #include &lt;GL/glut.h&gt; 改为 #include &lt;GLUT/GLUT.h&gt;</li></ul></li><li>Build Phases中改为</li></ul><img src="./03-02.png" alt="03-02 修改后的库增加" style="zoom:75%;"><ul><li><p><strong>运行结果</strong></p><img src="./03-03.png" alt="03-03 运行结果" style="zoom:30%;"><ul><li>关于FreeGLUT的问题，等之后遇到无法解决时再加以考虑了，汗汗，目前只找到<a href="https://blog.csdn.net/fqrq88918329/article/details/50154863" title="MAC OS上使用OpenGL遇到的大坑 -- 热心的李大妈" target="_blank" rel="noopener">[4]</a>。</li></ul></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]: <a href="https://learnopengl-cn.github.io/01%20Getting%20started/04%20Hello%20Triangle/" target="_blank" rel="noopener">https://learnopengl-cn.github.io/01%20Getting%20started/04%20Hello%20Triangle/</a>    “LearnOpenGL CN - 你好，三角形”<br><a href="https://www.cnblogs.com/fanghao/p/7559768.html" title="Mac使用Xcode配置openGL -- 潇雨危栏" target="_blank" rel="noopener">2</a>: <a href="https://www.cnblogs.com/fanghao/p/7559768.html" target="_blank" rel="noopener">https://www.cnblogs.com/fanghao/p/7559768.html</a>    “Mac使用Xcode配置openGL – 潇雨危栏”<br><a href="https://www.cnblogs.com/leojason/p/9619193.html" title="OpenGL学习之旅01—Xcode+OpenGL环境配置 -- LeoJason" target="_blank" rel="noopener">3</a>: <a href="https://www.cnblogs.com/leojason/p/9619193.html" target="_blank" rel="noopener">https://www.cnblogs.com/leojason/p/9619193.html</a>    “OpenGL学习之旅01—Xcode+OpenGL环境配置 – LeoJason”<br><a href="https://blog.csdn.net/fqrq88918329/article/details/50154863" title="MAC OS上使用OpenGL遇到的大坑 -- 热心的李大妈" target="_blank" rel="noopener">4</a>: <a href="https://blog.csdn.net/fqrq88918329/article/details/50154863" target="_blank" rel="noopener">https://blog.csdn.net/fqrq88918329/article/details/50154863</a>    “MAC OS上使用OpenGL遇到的大坑 – 热心的李大妈”</p>]]></content>
      
      
      <categories>
          
          <category> OpenGL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>iPad软件个人推荐</title>
      <link href="/else-2019-01-ipadsoftware/"/>
      <url>/else-2019-01-ipadsoftware/</url>
      
        <content type="html"><![CDATA[<h1 id="iPad软件个人推荐"><a href="#iPad软件个人推荐" class="headerlink" title="iPad软件个人推荐"></a>iPad软件个人推荐</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><p>用iPad来做学习工具已经一个学习多了，鉴于身边有意向购买、使用iPad的人似乎日渐增多，写下此篇小文章来介绍一下我日常使用的几款不错的软件。另外，本人的设备包含iPad（第六代）和Pencil（一代，二代不支持iPad Air）。下面是iPad和pencil的照片（随便乱拍，不要介意，汗汗），另外是不是黑框的比白框的显得更大气、神秘、好看呢，吼吼～</p><p><img src="./01-01.png" alt="01-01 iPad照片"></p></li><li><p>好的，开始正文，本文介绍的软件主要是以下六款（<em>Notability</em>，<em>MindNode</em>，<em>Kindle</em>，<em>PDF Expert</em>，<em>Agenda</em>和iPad自带的<em>日历</em>），<strong>其中Notability, MindNode 和 PDF Expert 都是收费的软件哦</strong>。</p><p><img src="./01-02.png" alt="01-02 软件图标显示"></p></li></ul><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="1-Notability"><a href="#1-Notability" class="headerlink" title="1. Notability"></a>1. Notability</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><ul><li><em>Notability</em> 主要作为一个<strong>笔记软件</strong>，搭配Pencil能发挥它的价值。不过由于我的课堂笔记一般用<em>MindNode</em>记成思维导图的形式，一些学习笔记则在电脑上用<em>Markdown</em>来记录，所以我的<em>Notability</em>主要用来写留言、打草稿、做会议记录之类的草记。</li></ul><h4 id="喜欢的点"><a href="#喜欢的点" class="headerlink" title="喜欢的点"></a>喜欢的点</h4><ul><li>框选复制：导出后，可以直接粘贴到其它软件变为图片格式，对于一些无法打出来的内容（如公式）还是很方便的选择；</li><li>画笔直线、圆形：长按停留后可以直接变为直线或圆形；</li><li>支持PDF格式：<em>Notability</em>中也可以导入pdf格式图片；</li></ul><h4 id="可能的局限"><a href="#可能的局限" class="headerlink" title="可能的局限"></a>可能的局限</h4><ul><li>无法输入公式，只能手写（不支持MarkDown）；</li></ul><h4 id="软件截图"><a href="#软件截图" class="headerlink" title="软件截图"></a>软件截图</h4><ul><li><p>下图左图是界面目录的截图，右侧是这学期的计算机图形学课程的图形处理流程的草图。</p><p><img src="./02-01.png" alt="02-01 Notability"></p></li></ul><h3 id="2-MindNode"><a href="#2-MindNode" class="headerlink" title="2. MindNode"></a>2. MindNode</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><ul><li><em>MindNode</em>是一款<strong>思维导图工具软件</strong>，帮助绘制思维导图。不过现在App Store中也有了一些新的免费的思维导图软件，像<em>XMind</em>之类的，不过绘制思维导图而言，<em>MindNode</em>是完全够用的，不过如果是新用户，可以先观察一下其它的软件，再做衡量。</li></ul><h4 id="喜欢的点-1"><a href="#喜欢的点-1" class="headerlink" title="喜欢的点"></a>喜欢的点</h4><ul><li>方便、直接的文件夹管理；</li><li>容易、便捷的绘制思维导图操作；</li><li>可以通过iCloud与Mac传输，可以导出其它格式；</li></ul><h4 id="可能的局限-1"><a href="#可能的局限-1" class="headerlink" title="可能的局限"></a>可能的局限</h4><ul><li>无法输入公式，只能以截图格式实现；</li><li>无法在思维导图外写入文本，有提供注释，但没法自己随意写；</li><li>思维导图铺得比较开，不够紧凑，感觉上会有很多闲置空间（效果看下图）；</li></ul><h4 id="软件截图-1"><a href="#软件截图-1" class="headerlink" title="软件截图"></a>软件截图</h4><ul><li><p>下图为<em>MindNode</em>的目录展示：</p><p><img src="./02-02.png" alt="02-02 MindNode 目录展示"></p></li><li><p>下图仍是计算机图形学课程第一章的思维导图展示（其中的加框图片就是从Notability中复制进来的）：</p><p><img src="./02-03.png" alt="02-03 MindNode 绘制图展示"></p></li></ul><h3 id="3-Kindle"><a href="#3-Kindle" class="headerlink" title="3. Kindle"></a>3. Kindle</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><ul><li><strong>电子书阅读器</strong>这个就不用说了，在亚马逊上购买好后，关联账户就可以直接阅读了，方便、快捷、省钱。是否购买主要还是看个人是否对亚马逊的电子书有需求（本人好多书都只看了一小部分，然后就结束了，汗颜）。</li></ul><h4 id="软件截图-2"><a href="#软件截图-2" class="headerlink" title="软件截图"></a>软件截图</h4><ul><li><p>下图，左侧为打开目录，右侧为《高性能MySQL》中的一页。</p><p><img src="./02-04.png" alt="02-04 Kindle"></p></li></ul><h3 id="4-PDF-Expert"><a href="#4-PDF-Expert" class="headerlink" title="4. PDF Expert"></a>4. PDF Expert</h3><h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><ul><li><strong>PDF阅读器</strong>，论文、PDF书籍的绝佳阅读软件。</li></ul><h4 id="喜欢的点-2"><a href="#喜欢的点-2" class="headerlink" title="喜欢的点"></a>喜欢的点</h4><ul><li>管理方便，注释、圈画便利，支持pencil直接手写；</li><li>可以双页展示、垂直滚动、水平翻滚；</li></ul><h4 id="可能的局限-2"><a href="#可能的局限-2" class="headerlink" title="可能的局限"></a>可能的局限</h4><ul><li>当PDF不是文件而是图片的时候，标记加下划线就只能一行一行画横线了（其实也还好啦）；</li></ul><h4 id="软件截图-3"><a href="#软件截图-3" class="headerlink" title="软件截图"></a>软件截图</h4><ul><li><p>同样的，左侧是我的Article文件夹下的目录情况，右图是其中一篇论文的打开情况。</p><p><img src="./02-05.png" alt="02-05 PDF Expert"></p></li></ul><h3 id="5-Agenda"><a href="#5-Agenda" class="headerlink" title="5. Agenda"></a>5. Agenda</h3><h4 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h4><ul><li>制定计划、目标、日志的软件，个人的话，目前日历软件取代了<em>Agenda</em>的一部分功能。</li></ul><h4 id="喜欢的点-3"><a href="#喜欢的点-3" class="headerlink" title="喜欢的点"></a>喜欢的点</h4><ul><li>免费！免费的！</li><li>可以方便的放目标（打勾的图框）；</li><li>能与日历关联，选择一天、一周还是其它的时长；</li></ul><h4 id="可能的局限-3"><a href="#可能的局限-3" class="headerlink" title="可能的局限"></a>可能的局限</h4><ul><li>主体的语言是英语（当然自己可以输入英文）；</li></ul><h4 id="软件截图-4"><a href="#软件截图-4" class="headerlink" title="软件截图"></a>软件截图</h4><ul><li><p>上面是一周的计划，下面是单日的。</p><p><img src="./02-06.png" alt="02-06 Agenda"></p></li></ul><h3 id="6-日历"><a href="#6-日历" class="headerlink" title="6. 日历"></a>6. 日历</h3><h4 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h4><ul><li>iPad自带的日历软件，最近几周才开始使用，才发现它的魅力——促进时间利用、督促自己（我比较懒，会浪费很多时间）。</li></ul><h4 id="喜欢的点-4"><a href="#喜欢的点-4" class="headerlink" title="喜欢的点"></a>喜欢的点</h4><ul><li>可以与mac、iphone直接关联提醒（由于我妈的iPhone用的是我的Apple ID，所以她也看得到我的，嘤嘤）；</li><li>方便设置时间段的安排，能督促学习，吼吼；</li></ul><h4 id="可能的局限-4"><a href="#可能的局限-4" class="headerlink" title="可能的局限"></a>可能的局限</h4><ul><li>没有更多的功能？</li></ul><h4 id="软件截图-5"><a href="#软件截图-5" class="headerlink" title="软件截图"></a>软件截图</h4><ul><li><p>下图就是展示了啦，安排、安排！（最近几天又开始犯懒不安排了，这不又弄到了凌晨了不是，太难了QAQ）</p><p><img src="./02-07.png" alt="02-07 日历的打开方式"></p></li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ul><li>再说一次，<strong>其中Notability, MindNode 和 PDF Expert 都是收费的软件，收费的软件，收费的软件哦！</strong>另外，这些软件都是直接从App Store中下载的哦。</li><li>另外，软件再好，iPad再好，看了这篇文章你再心动，也未必有用哦！因为没有“用来学习”的觉悟与决心，或许你的iPad最后只能沦为一个看视频、玩游戏的设备了，很多人都这样，不用灰心，反正比手机屏幕大，看视频、玩游戏更快乐不是，嘿嘿～</li><li>最后一件事，国庆快乐～<strong>目标是星辰大海！</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> Else </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ipad </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenGL学习笔记（一）简介&amp;初探</title>
      <link href="/opengl-01/"/>
      <url>/opengl-01/</url>
      
        <content type="html"><![CDATA[<h1 id="OpenGL学习笔记（一）简介-amp-初探"><a href="#OpenGL学习笔记（一）简介-amp-初探" class="headerlink" title="OpenGL学习笔记（一）简介&amp;初探"></a>OpenGL学习笔记（一）简介&amp;初探</h1><h2 id="1-OpenGL"><a href="#1-OpenGL" class="headerlink" title="1. OpenGL"></a>1. OpenGL</h2><h3 id="OpenGL-简介"><a href="#OpenGL-简介" class="headerlink" title="OpenGL 简介"></a>OpenGL 简介</h3><ul><li><strong>OpenGL</strong>（Open Graphics Library，开放式图形库）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序接口（API），包含了一系列可以操作图形、图像的函数。<a href="https://baike.baidu.com/item/OpenGL/238984?fr=aladdin" title="OpenGL-百度百科" target="_blank" rel="noopener">[1]</a><a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li><li>OpenGL官网的描述其为”<em>The Industry’s Foundation for High Performance</em>“<a href="https://www.opengl.org" title="OpenGL 官网" target="_blank" rel="noopener">[3]</a>，OpenGL本身并不是一个API，仅仅是一个<strong>由Khronos组织制定并维护的规范</strong>（Specification）。OpenGL规范严格规定了每个函数如何运行，以及其输出输出值，而函数的具体实现，则由OpenGL库的开发者自行决定。<a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li><li>实际上，通常是<strong>显卡的生产商</strong>开发具体的OpenGL库，如使用Apple系统时，OpenGL库是由Apple自身维护。当OpenGL产生bug时，可以通过<strong>升级显卡驱动</strong>解决。<a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li></ul><h3 id="OpenGL-相关库"><a href="#OpenGL-相关库" class="headerlink" title="OpenGL 相关库"></a>OpenGL 相关库</h3><ul><li><strong>OpenGL core Library</strong><ul><li>OpenGL核心库</li></ul></li><li><strong>OpenGL Utility Library</strong>（GLU）</li><li><strong>OpenGL Utility Toolkit</strong>（GLUT）<ul><li>提供窗口系统的功能命令</li></ul></li><li><strong>freeglut</strong><ul><li>更新OpenGL的库</li></ul></li><li><strong>OpenGL Extension Wrangler Library</strong>（GLEW）</li></ul><h3 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h3><ul><li>1980s，软件开发人员为每种图形硬件编写自定义的借口和驱动程序。</li><li>1990初，SGI成为工作站3D图形领域的领导者，其IRIS GL API成为事实上的行业标准。其竞争对手（Sun、惠普、IBM）通过扩展PHIGS标准也将3D硬件投入市场，SCI 将 IRIS GL API转变为一项开放标准，即OpenGL。</li><li>1992年，SCI领导OpenGL架构审查委员会（OpenGL ARB）的创建。</li><li>1995年，微软发布Direct3D成为OpenGL的主要竞争对手。</li><li>1997年12月17日，微软和SGI发起华氏温标项目，统一OpenGL和Direct3D接口。1998年，惠普加入，但项目于1999年停止。</li><li>2006年7月，OpenGL架构评审委员会将OpenGL API标准的控制权交给Khronos Group。 </li></ul><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ul><li>此部分主要参考<a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li></ul><h4 id="核心模式-与-立即渲染模式"><a href="#核心模式-与-立即渲染模式" class="headerlink" title="核心模式 与 立即渲染模式"></a>核心模式 与 立即渲染模式</h4><ul><li><strong>立即渲染模式</strong>（Immediate mode, 固定渲染管线）：早期OpenGL使用，绘制图形容易使用和理解，但是效率较低。</li><li><strong>核心模式</strong>（Core-profile）：从OpenGL3.2后开始使用，移除了旧的特性，迫使用户使用现代的函数。</li></ul><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><ul><li><strong>扩展</strong>（Extension）：支持扩展，未成为规范之前，显卡公司提出的新特性或优化可以通过扩展的方式在驱动中实现。</li></ul><h4 id="状态机"><a href="#状态机" class="headerlink" title="状态机"></a>状态机</h4><ul><li>OpenGL自身是一个巨大的<strong>状态机</strong>（State Machine）：一系列的变量描述此时的OpenGL如何运行。OpenGL的状态通常被称为OpenGL上下文。</li><li>使用OpenGL时，使用<strong>状态设置函数</strong>（State-changing Function）与 <strong>状态使用函数</strong>（State-using Function）来进行操作。</li></ul><h2 id="2-初次尝试"><a href="#2-初次尝试" class="headerlink" title="2. 初次尝试"></a>2. 初次尝试</h2><ul><li>以下内容针对Mac上的Xcode（版本 Xcode11），其他平台的搭建还请参考网上的其他资源（MAC XCODE已自带GLUT.framework和OpenGL.framework，所以初次尝试不涉及“复杂”的环境配置）。以下内容参考<a href="https://www.cnblogs.com/chenyangsocool/p/5357691.html" title="Max Xcode 下配置OpenGL - chenyangsocool - 博客园" target="_blank" rel="noopener">[4]</a></li></ul><ol><li><p>打开Xcode，新建一个Command Line Tool项目。</p><p><img src="./02-01.png" alt="01-01 新建Command Line Tool项目"></p></li><li><p>添加GLUT.framework和OpenGL.framework</p><ol><li><p>点击Build Phases, Link Binary with Libraries</p><p><img src="./02-02.png" alt="01-02 添加库位置"></p></li><li><p>寻找、添加GLUT.framework和OpenGL.framework</p><p><img src="./02-03.png" alt="01-03 添加framework"></p></li><li><p>完成后效果</p><p><img src="./02-04.png" alt="01-04 添加后效果"></p></li></ol></li><li><p>进入main.cpp编写Demo代码并运行</p><ol><li><p>Demo代码</p><pre class=" language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">//绘制一个正方形</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;GLUT/GLUT.h></span></span><span class="token keyword">void</span> <span class="token function">myDisplay</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">glClear</span><span class="token punctuation">(</span>GL_COLOR_BUFFER_BIT<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glRectf</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5f</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5f</span><span class="token punctuation">,</span> <span class="token number">0.5f</span><span class="token punctuation">,</span> <span class="token number">0.5f</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glFlush</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">glutInit</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>argc<span class="token punctuation">,</span> argv<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitDisplayMode</span><span class="token punctuation">(</span>GLUT_RGB <span class="token operator">|</span> GLUT_SINGLE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitWindowPosition</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitWindowSize</span><span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutCreateWindow</span><span class="token punctuation">(</span><span class="token string">"第一个 OpenGL 程序"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutDisplayFunc</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>myDisplay<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutMainLoop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li><p>运行效果</p><img src="./02-05.png" alt="01-05" style="zoom:30%;"></li><li><p>由于在macOS 10.9、10.14 后一些命令被废止，会出现以下提示信息，下一篇会使用GLEW库。</p><p><img src="./02-06.png" alt="01-06 XCODE中的提示"></p></li></ol></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]: <a href="https://baike.baidu.com/item/OpenGL/238984?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/OpenGL/238984?fr=aladdin</a>    “OpenGL-百度百科”<br>[2]: <a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" target="_blank" rel="noopener">https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/</a>    “LearnOpenGL CN-入门”<br>[3]: <a href="https://www.opengl.org" target="_blank" rel="noopener">https://www.opengl.org</a>    “OpenGL 官网”<br>[4]: <a href="https://www.cnblogs.com/chenyangsocool/p/5357691.html" target="_blank" rel="noopener">https://www.cnblogs.com/chenyangsocool/p/5357691.html</a>    “Max Xcode 下配置OpenGL - chenyangsocool - 博客园”</p>]]></content>
      
      
      <categories>
          
          <category> OpenGL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读笔记&lt;03&gt;</title>
      <link href="/article-2019-03/"/>
      <url>/article-2019-03/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-论文阅读笔记-lt-03-gt"><a href="#2019-论文阅读笔记-lt-03-gt" class="headerlink" title="2019 论文阅读笔记 &lt;03&gt;"></a>2019 论文阅读笔记 &lt;03&gt;</h1><h2 id="【基本信息】"><a href="#【基本信息】" class="headerlink" title="【基本信息】"></a>【基本信息】</h2><ul><li><strong>论文题目：</strong><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" target="_blank" rel="noopener">Attention is all you need</a></li><li><strong>作者：</strong>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Adian N. Gomez, Łukasz Kaiser, Illia Polosukhin </li><li><strong>出版：</strong> <a href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017" target="_blank" rel="noopener">Advances in Neural Information Processing Systems 30 (NIPS 2017)</a></li><li><strong>日期：</strong>Submitted on 12 Jun 2017 （*<a href="https://arxiv.org/abs/1706.03762v1" target="_blank" rel="noopener">v1</a>*）, last revised 6 Dec 2017 （this version, v5）</li><li><strong>简介：</strong>此论文针对以往NLP中使用RNN结构和endoder-decoder结构无法并行、速度慢的问题，基于<strong>注意力机制</strong>提出了一种称为<strong>Transformer</strong>的网络结构。</li></ul><h2 id="【词汇术语】"><a href="#【词汇术语】" class="headerlink" title="【词汇术语】"></a>【词汇术语】</h2><h3 id="专业词汇-术语"><a href="#专业词汇-术语" class="headerlink" title="专业词汇/术语"></a>专业词汇/术语</h3><ul><li><strong>sequence transduction models</strong></li><li><strong>attention mechanism:</strong> 注意力机制</li></ul><h3 id="语言词汇"><a href="#语言词汇" class="headerlink" title="语言词汇"></a>语言词汇</h3><ul><li><strong>push the boundaries</strong></li><li><strong>integral</strong> 必需的</li></ul><h3 id="模型-模式-算法"><a href="#模型-模式-算法" class="headerlink" title="模型/模式/算法"></a>模型/模式/算法</h3><ul><li><strong>softmax</strong><ul><li>softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类</li><li>$S_i=\frac{e^i}{\sum_je^j}$</li></ul></li></ul><h2 id="【论文笔记】"><a href="#【论文笔记】" class="headerlink" title="【论文笔记】"></a>【论文笔记】</h2><h3 id="「简介前言」"><a href="#「简介前言」" class="headerlink" title="「简介前言」"></a>「简介前言」</h3><h4 id="当前缺陷"><a href="#当前缺陷" class="headerlink" title="当前缺陷"></a>当前缺陷</h4><ul><li><strong>RNN</strong><ul><li>前后状态的依赖性导致无法并行，在较长的词序时运行缓慢</li></ul></li><li><strong>当前努力</strong><ul><li>分解（factorization tricks）, 条件计算（conditional computation）</li><li><em>Neural GPU<em>，</em>ByteNet</em> 和 <em>ConvS2S</em></li><li>按序计算的限制仍然存在</li></ul></li></ul><h4 id="注意力机制（Attention-Mechanisms）"><a href="#注意力机制（Attention-Mechanisms）" class="headerlink" title="注意力机制（Attention Mechanisms）"></a>注意力机制（Attention Mechanisms）</h4><ul><li><strong>注意力机制</strong>（Attention Mechanisms）<ul><li>各种序列模型建模和转换模型中的一个组成部分（<em>an integral part of compelling sequence modeling and transduction models in various tasks</em>）</li><li>可以不用考虑在输入输出序列中的距离而允许建模</li><li>在少量情况，和循环网络结合使用</li></ul></li><li><strong>自注意力机制</strong>（Self-attention，intra-attention）<ul><li>把输入序列上不同位置的信息联系起来计算一个句子的表示内容</li></ul></li></ul><h3 id="「Transformer模型」"><a href="#「Transformer模型」" class="headerlink" title="「Transformer模型」"></a>「Transformer模型」</h3><h4 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h4><ul><li><p><strong>结构图</strong></p><p><img src="./03-01the-transformer-model-architecture.png" alt="03-01the-transformer-model-architecture"></p></li><li><p><strong>Encoder</strong></p><ul><li>由6个基本层堆叠</li><li>每层含有两个子层：<ul><li>注意力机制（<em>multi-head self-attention mechanism</em>）</li><li>全连接前向网络（<em>simple, position-wise fully connected feed-forward network</em>）</li></ul></li><li>在两个子层中都有剩余连接（<em>residual connection</em>）和层标准化（<em>layer normalization</em>）<ul><li>子层的输出：$LayerNorm(x+Sublayer(x))$</li></ul></li></ul></li><li><p><strong>Decoder</strong></p><ul><li>在encoder的输出外再增加了一层注意力机制</li><li>掩饰（<em>masking</em>）<ul><li>确保每个位置的预测只基于前面已知位置的词</li></ul></li></ul></li></ul><h4 id="注意力机制（Attention）"><a href="#注意力机制（Attention）" class="headerlink" title="注意力机制（Attention）"></a>注意力机制（Attention）</h4><ul><li><p><strong>注意力机制</strong></p><ul><li>将一个查找和一组键值对映射到正确的输出（<em>mapping a query and a set of key-value pairs to an output</em>）</li></ul><p><img src="./03-02tow-attention.png" alt="03-02tow-attention"></p></li><li><p><strong>Scaled Dot-Product Attention</strong></p><ul><li>$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</li><li>$d_k$：keys的维度；$d_v$：value的维度</li><li>$Q$：查询矩阵；$K$：keys的矩阵；$V$：values的矩阵</li></ul></li><li><p><strong>Multi-Head Attention</strong></p><ul><li>$MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^{\bigcirc}$</li><li>$head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$</li></ul></li><li><p>文章使用的注意力机制</p><ul><li>Multi-Head Attention，三个方面：</li><li>Encoder-Decoder Attention层<ul><li>查询来自前一个decoder层，允许decoder的每个位置都能关注输入的所有位置</li></ul></li><li>Encoder层中的Self-attention层<ul><li>在self-attention层中所有的key、value和query都来自前一层的encoder，encoder的每个位置都能关注前一层输出的所有位置。</li></ul></li><li>Decoder层中的Self-attention层</li></ul></li></ul><h4 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h4><ul><li>每一层都有一个<strong>fukky connected feed-forward network</strong><ul><li>$FNN(x)=\max(0,xW_1+b_1)W_2+b_2$</li><li>两个线性转换，中间一个ReLU</li></ul></li></ul><h4 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h4><h4 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h4><ul><li>不使用递归结构和卷积结构，引入位置编码来使用输入顺序信息</li><li><strong>位置编码：</strong><ul><li>$PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})$</li><li>$PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})$</li><li>$pos$: position; $i$: dimension</li></ul></li><li>使用sin版本，因为它允许序列用到更长</li></ul><h3 id="「为什么使用自注意力机制」"><a href="#「为什么使用自注意力机制」" class="headerlink" title="「为什么使用自注意力机制」"></a>「为什么使用自注意力机制」</h3><ul><li>三个方面比较其与递归结构、卷积结构<ul><li>每层的计算复杂度（<em>total computational complexity per layer</em>）</li><li>能被并行计算的数量（<em>the amount of computation that can be parallelized</em>）</li><li>网络中长范围以来的路径长度（<em>the path length between long-range dependencies in the network</em>）</li></ul></li></ul><h3 id="「实验与结论」"><a href="#「实验与结论」" class="headerlink" title="「实验与结论」"></a>「实验与结论」</h3><ul><li>同前文，不加以赘述，有兴趣可以自行查看论文以及相关代码。</li></ul><h2 id="「小记总结」"><a href="#「小记总结」" class="headerlink" title="「小记总结」"></a>「小记总结」</h2><ul><li>论文中为提高并行能力，引入注意力机制代替递归结构和卷积结构，用位置编码来体现句子顺序信息。换种思路实现位置表示，NICE～是否还有其他方法，或者还有其他的耗时处理可以用更为简洁的方法来代替处理呢？</li></ul>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读笔记&lt;02&gt;</title>
      <link href="/article-2019-02/"/>
      <url>/article-2019-02/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-论文阅读笔记-lt-02-gt"><a href="#2019-论文阅读笔记-lt-02-gt" class="headerlink" title="2019 论文阅读笔记 &lt;02&gt;"></a>2019 论文阅读笔记 &lt;02&gt;</h1><h2 id="【基本信息】"><a href="#【基本信息】" class="headerlink" title="【基本信息】"></a>【基本信息】</h2><ul><li><strong>论文题目：</strong><a href="https://www.aclweb.org/anthology/D14-1181/" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></li><li><strong>作者：</strong>Yoon Kim</li><li><strong>组织：</strong>New York University</li><li><strong>出版：</strong>Association for Computational Linguistics</li><li><strong>关联：</strong>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</li><li><strong>日期：</strong>October 2014</li><li><strong>简介：</strong>此论文在CNN上使用词向量处理自然语言的问题，将词向量与深度学习（NLP与卷积神经网络）结合，并构建了一个简单的CNN模型进行实验，指出了词向量是神经网络对于自然语言处理的一个重要部分<em>（unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP）</em>。</li></ul><h2 id="【词汇术语】"><a href="#【词汇术语】" class="headerlink" title="【词汇术语】"></a>【词汇术语】</h2><h3 id="专业词汇-术语"><a href="#专业词汇-术语" class="headerlink" title="专业词汇/术语"></a>专业词汇/术语</h3><ul><li><strong>tanh</strong>: hyperbolic tangent, 双曲正切函数</li><li><strong>pooling:</strong> 池化层，集中特征</li><li><strong>Regularization:</strong> 正则化<ul><li>给损失函数加一个正则化项</li><li><strong>l2-norms</strong></li></ul></li></ul><h3 id="语言词汇"><a href="#语言词汇" class="headerlink" title="语言词汇"></a>语言词汇</h3><ul><li><strong>utilize:</strong> 利用</li><li><strong>disentangle:</strong> 摆脱</li></ul><h3 id="模型-模式-算法"><a href="#模型-模式-算法" class="headerlink" title="模型/模式/算法"></a>模型/模式/算法</h3><ul><li>$\circ$: the element-wise multiplication operator<ul><li>数组对应元素相乘</li></ul></li><li><strong>Bernoulli random variables</strong>：伯努利随机变量<ul><li>01分布：$P(X=1)=p,\ P(x=1)=1-p$</li></ul></li><li><strong>norm:</strong> 范数<ul><li>$l_2-norms(||w||_2)$: 第二范数，欧几里得范数<ul><li>$||w||_2=\sqrt{x_1^2+x_2^2+…+x_n^2}$</li></ul></li></ul></li></ul><h2 id="【论文笔记】"><a href="#【论文笔记】" class="headerlink" title="【论文笔记】"></a>【论文笔记】</h2><h3 id="「简介内容」"><a href="#「简介内容」" class="headerlink" title="「简介内容」"></a>「简介内容」</h3><ul><li><strong>深度学习</strong>（Deep Learning）：当时主要应用于计算机视觉、语音识别，在自然语言处理方面，则用于特征分类上。</li><li><strong>词向量</strong>（Word Vectors）</li><li><strong>卷积神经网络</strong>（Convolutional neural networks, CNN）<ul><li>利用卷积过滤器层</li><li>最早用于计算机视觉</li><li>在语义分析上展现出能力</li></ul></li></ul><h3 id="「模型」"><a href="#「模型」" class="headerlink" title="「模型」"></a>「模型」</h3><h4 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图    "></a>模型结构图    <img src="./02-01cnn-model-architecture.png" alt="CNN Model Architecture"></h4><h4 id="变量信息"><a href="#变量信息" class="headerlink" title="变量信息"></a>变量信息</h4><ul><li>$x_i$: $k$维词向量句子中的第$i$个词语</li><li>$x_{1:n}$: 长度为n的句子</li><li>$w$: 长度为$h$的窗口</li><li>特征: $c_i=f(w\cdot x_{i:i+h-1}+b)$<ul><li>$f$: 非线性函数</li><li>$b$: 偏移</li></ul></li><li>$\hat{c}=\max{c}$</li></ul><h4 id="各层信息"><a href="#各层信息" class="headerlink" title="各层信息"></a>各层信息</h4><ul><li><strong>池化层</strong>：max-pooling方法</li><li><strong>全连接层</strong>：softmax</li></ul><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul><li>正则化用于防止过拟合，常见是在损失函数中加入正则项（L1、L2正则化），本文采用的是DropOut方法。</li><li><strong>Dropout</strong><ul><li>训练中随机删掉隐藏层的一半节点进行学习，之后再对另一半节点进行一轮学习，最终将权重除以2。</li><li>减少了神经元间的依赖性，降低了过拟合，提高了准确率。</li></ul></li></ul><h3 id="「实验结论」"><a href="#「实验结论」" class="headerlink" title="「实验结论」"></a>「实验结论」</h3><ul><li>实验方法、结果不再重复，有兴趣可以自行阅读论文。</li><li>最重要的一点是，该论文指出词向量是神经网络对于自然语言处理的一个重要部分（<em>Our results add to the well-established evidence that unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP.</em>）。</li></ul><h2 id="【小记总结】"><a href="#【小记总结】" class="headerlink" title="【小记总结】"></a>【小记总结】</h2><ul><li>此论文作为CNN与NLP结合的开山之作，虽然模型简单，但却极具价值和意义。创造性、拓展性的思维和能力，急需。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读笔记&lt;01&gt;</title>
      <link href="/article-2019-01/"/>
      <url>/article-2019-01/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-论文阅读笔记-lt-01-gt"><a href="#2019-论文阅读笔记-lt-01-gt" class="headerlink" title="2019 论文阅读笔记 &lt;01&gt;"></a>2019 论文阅读笔记 &lt;01&gt;</h1><h2 id="【基本信息】"><a href="#【基本信息】" class="headerlink" title="【基本信息】"></a>【基本信息】</h2><ul><li><strong>论文题目：</strong> <em><a href="https://ieeexplore.ieee.org/document/7372153" target="_blank" rel="noopener">Entropy-Based Term Weighting Schemes for Text Categorization in VSM</a></em></li><li><strong>作者：</strong>Tao Wang ; Yi Cai ; Ho-fung Leung ; Zhiwei Cai ; Huaqing Min</li><li><strong>出版：</strong> <a href="https://ieeexplore.ieee.org/xpl/conhome/7372093/proceeding" target="_blank" rel="noopener">2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)</a></li><li><strong>日期：</strong>9-11 Nov. 2015 / 07 Jan 2016</li><li><strong>标签：</strong>TC、VSM</li><li><strong>简介：</strong>此论文分析了用于文本分类任务的一些权重法（如无监督学习中的$tf,tf\cdot idf,BM25$ 与监督学习中的$rf,iqf\cdot qf\cdot icf,tf\cdot gr$等），指出这些已有模式在文本分类中存在的问题，并提出了两种新的基于熵的权重法（$tf\cdot dc$ 和 $tf\cdot bdc$），提升了术语的辨别力与文本分类任务的完成。</li></ul><h2 id="【词汇术语】"><a href="#【词汇术语】" class="headerlink" title="【词汇术语】"></a>【词汇术语】</h2><h3 id="专业词汇-术语"><a href="#专业词汇-术语" class="headerlink" title="专业词汇/术语"></a>专业词汇/术语</h3><ul><li><strong>TC</strong> （Text Categorization）：文本分类</li><li><strong>VSM</strong> （Vector Space Model）：向量空间模型</li><li><strong>IR</strong> （Information Retrieval）：信息检索</li><li><strong>unsupervised</strong> &amp; <strong>supervised</strong>：无监督、监督</li><li><strong>Contingency Table</strong> 情形分析表<ul><li><strong>positive category （PC）</strong> ：正类</li><li><strong>negative category （NC）</strong>：负类</li></ul></li></ul><h3 id="语言词汇"><a href="#语言词汇" class="headerlink" title="语言词汇"></a>语言词汇</h3><ul><li><strong>state-of-the-art</strong>：当前最好的</li></ul><h3 id="模型-模式-算法"><a href="#模型-模式-算法" class="headerlink" title="模型/模式/算法"></a>模型/模式/算法</h3><ul><li><strong>KNN</strong> （k-Nearest Neighbor）：邻近算法</li><li><strong>SVM</strong> （Support Vector Machine）：支持向量机</li><li>$tf$：<strong>Term Frequency</strong> 词频<ul><li>$词频(TF)=某个词在文章中的出现次数$</li><li>$词频(TF)=\frac{某个词在文章中的出现次数}{文章的总词数}$</li><li>$词频(TF)=\frac{某个词在文章中的出现次数}{该文出现次数最多的词的出现次数}$</li><li>Variants: $\log(tf),\log(tf+1),log(tf)+1,…$</li></ul></li><li>$idf$: <strong>Inverse document frequency</strong> 逆文本频率指数<ul><li>$逆文档频率(IDF)=\log(\frac{语料库的文档总数}{包含该词的文档数+1})$</li><li>$df$: <strong>document frequency</strong></li><li>包含词条的文档越小，$idf$越大</li></ul></li><li>$dc$: <strong>distributional concentration</strong><ul><li>$dc(t)=1-\frac{H(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{f(t,c_i)}{f(t)}\log{\frac{f(t,c_i)}{f(t)}}}{\log{|C|}}$</li></ul></li><li>$rf$: <strong>relevance frequency</strong>, $rf=\frac{a}{c}$</li><li>$cf$: <strong>category frequency</strong>, 出现的类别越少，值越大</li><li>$bdc$: <strong>balanced distributional concentration</strong><ul><li>$bdc(t)=1-\frac{BH(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{p(t|c_i)}{\sum^{|C|}_{i=1}p(t|c_i)}\log{\frac{p(t|c_i)}{sum^{|C|}_{i=1}p(t|c_i)}}}{\log{|C|}}$</li></ul></li></ul><h2 id="【论文笔记】"><a href="#【论文笔记】" class="headerlink" title="【论文笔记】"></a>【论文笔记】</h2><h3 id="「已有方法-—-无监督学习方法」"><a href="#「已有方法-—-无监督学习方法」" class="headerlink" title="「已有方法 —- 无监督学习方法」"></a>「已有方法 —- 无监督学习方法」</h3><h4 id="无监督学习（unsupervised）"><a href="#无监督学习（unsupervised）" class="headerlink" title="无监督学习（unsupervised）"></a>无监督学习（unsupervised）</h4><ul><li><strong>含义：</strong>无先验知识（无标签）的学习</li><li><strong>常见模式：</strong>$tf,tf\cdot idf,BM25 $</li></ul><h4 id="缺点问题"><a href="#缺点问题" class="headerlink" title="缺点问题"></a>缺点问题</h4><ul><li><strong>【由于】</strong>关注于词出现的次数，而忽略了训练<em>文档（documents）<em>的</em>类别标签（category labels）</em></li><li><strong>【导致】</strong><ul><li><em>词语（term）</em>能区别文档的差异，但不能区别类别的差异</li><li>在文本分类任务中，不足以衡量词语对文档类别的<em>辨别能力（discriminating power）</em></li></ul></li></ul><h3 id="「已有方法-—-监督学习方法」"><a href="#「已有方法-—-监督学习方法」" class="headerlink" title="「已有方法 —- 监督学习方法」"></a>「已有方法 —- 监督学习方法」</h3><h4 id="监督学习（supervised）"><a href="#监督学习（supervised）" class="headerlink" title="监督学习（supervised）"></a>监督学习（supervised）</h4><ul><li><strong>含义：</strong>利用已知类别的样本进行学习</li><li><strong>常见模式：</strong>$rf,iqf\cdot qf\cdot icf,tf\cdot gr$</li></ul><h4 id="缺点问题-1"><a href="#缺点问题-1" class="headerlink" title="缺点问题"></a>缺点问题</h4><ul><li><strong>缘由：</strong>大部分监督学习用到了情形分析表中的正类（PC）和负类（NC）</li><li><strong>【由于】</strong>在<em>多类别的情况（multi-class case）</em>中，正类只有一个类，而负类是多个类的集合</li><li><strong>【导致】</strong>负类中产生了<em>信息损失（information loss）</em></li></ul><h4 id="已有优化仍然存在的问题"><a href="#已有优化仍然存在的问题" class="headerlink" title="已有优化仍然存在的问题"></a>已有优化仍然存在的问题</h4><ul><li><em>正类和负类的分离问题 （PC/NC-split based schemes）</em>仍然存在，使得无法有效区别类别。</li><li><em>一种基于统计置信区间的模式（a scheme based on statistical confidence intervals）</em><ul><li>过于复杂难以实现</li></ul></li></ul><h3 id="「基于熵的权重法」"><a href="#「基于熵的权重法」" class="headerlink" title="「基于熵的权重法」"></a>「基于熵的权重法」</h3><h4 id="基于熵的权重法（entropy-based-term-weighting-schemes）"><a href="#基于熵的权重法（entropy-based-term-weighting-schemes）" class="headerlink" title="基于熵的权重法（entropy- based term weighting schemes）"></a>基于熵的权重法（entropy- based term weighting schemes）</h4><ul><li><strong>观点</strong><ul><li>利用负类中的具体类别帮助提升辨别力</li><li><em>更高浓度（higher concentration）</em>的词语具有更强的辨别能力</li><li>在类别层面有更高浓度分布的词语的熵更小</li></ul></li><li>熵值越小，辨别能力越强</li></ul><h4 id="模式1-Distribution-Concentration"><a href="#模式1-Distribution-Concentration" class="headerlink" title="模式1: Distribution Concentration"></a>模式1: Distribution Concentration</h4><ul><li><strong>模式</strong>：$dc$, distribution concentration</li><li><strong>计算公式：</strong> $dc(t)=1-\frac{H(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{f(t,c_i)}{f(t)}\log{\frac{f(t,c_i)}{f(t)}}}{\log{|C|}}$<ul><li>$|C|$: 类别数量</li><li>$f(t,c_i)$: 表示词语 $t$ 在类别 $c_i$ 中出现的频率，这里使用 $df(t,c_i)$</li><li>$f(t)$: 词语 $t$ 在所有类别出现的频数和</li><li>$H(t)$: 词语 $t$ 在语料库中类别的熵，$H(t)\in [0,\log|C|]$</li></ul></li><li><strong>特征优势</strong><ul><li>词语的权重基于在类别中词语的全局分布，而不是依赖于已有的类别正类。</li><li>因此，不需要已有的正类标签就可以进行分类。</li></ul></li><li><strong>缺陷</strong><ul><li>缺少类别的优先级信息（不同种类文档的长度不同，会导致熵的计算产生偏差）<ul><li>$bdc$模式解决此问题</li></ul></li></ul></li></ul><h4 id="模式2-Balanced-Distributional-Concentration"><a href="#模式2-Balanced-Distributional-Concentration" class="headerlink" title="模式2: Balanced Distributional Concentration"></a>模式2: Balanced Distributional Concentration</h4><ul><li><strong>模式</strong>：$bdc$, balanced distributional concentration</li><li><strong>计算公式：</strong> $bdc(t)=1-\frac{BH(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{p(t|c_i)}{\sum^{|C|}_{i=1}p(t|c_i)}\log{\frac{p(t|c_i)}{sum^{|C|}_{i=1}p(t|c_i)}}}{\log{|C|}}$</li><li><strong>特征优势</strong><ul><li>解决了 $dc$ 模式存在的问题</li></ul></li></ul><h3 id="「实验结果」"><a href="#「实验结果」" class="headerlink" title="「实验结果」"></a>「实验结果」</h3><ul><li>此处对于实验的数据、内容、结论不做详细描述，有兴趣可以自行查看论文。</li></ul><h2 id="【小记总结】"><a href="#【小记总结】" class="headerlink" title="【小记总结】"></a>【小记总结】</h2><ul><li>2019论文博客整理的第一篇，希望能坚持，不足之处还请谅解。许多的英文词汇转为中文总有些怪异的感觉，就保留在中文词后面了。</li><li>此论文指出了过往文本分类任务中一些模式（scheme, 这个词翻译成模式也觉得读起来不太顺口，汗）的缺陷，无法很好地利用标签信息、区别类别差异，因而引入了熵的概念。一种角度来说，也是更充分地利用“压榨”已有信息（无监督学习模式忽略了类别标签，PC/NC的监督学习模式则忽略了NC中的类别差异），挖掘、压榨、充分利用全部信息，促进更好的分类。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读列表</title>
      <link href="/article-2019-00/"/>
      <url>/article-2019-00/</url>
      
        <content type="html"><![CDATA[<h1 id="论文阅读列表-2019"><a href="#论文阅读列表-2019" class="headerlink" title="论文阅读列表-2019"></a>论文阅读列表-2019</h1><h2 id="在读-已读列表"><a href="#在读-已读列表" class="headerlink" title="在读/已读列表"></a>在读/已读列表</h2><ol><li><a href="https://ieeexplore.ieee.org/document/7372153" target="_blank" rel="noopener">Entropy-Based Term Weighting Schemes for Text Categorization in VSM</a></li><li><a href="https://www.aclweb.org/anthology/D14-1181/" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></li><li><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" target="_blank" rel="noopener">Attention is All you Need</a></li><li><a href="http://kns.cnki.net/kcms/detail/61.1167.g3.20190910.1730.010.html" target="_blank" rel="noopener">黄炜,黄建桥,李岳峰.基于BiLSTM-CRF的涉恐信息实体识别模型研究</a></li><li>Neural Architectures for Named Entity Recognition</li><li>文本自动生成研究进展与趋势</li></ol><h2 id="候选列表"><a href="#候选列表" class="headerlink" title="候选列表"></a>候选列表</h2><ol><li>Man Lan, Chew Lim Tan, Jian Su, and Yue Lu. Supervised and traditional term weighting methods for automatic text categorization. <em>Pattern Analysis and Machine Intelligence, IEEE Transactions on</em>, 31(4):721–735, 2009.</li><li>Xinghua Lu, Bin Zheng, Atulya Velivelli, and ChengXiang Zhai. Enhancing text categorization with semantic-enriched representation and training data augmentation. <em>Journal of the American Medical Informatics Association</em>, 13(5):526–535, 2006.</li></ol>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
