<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>OpenGL学习笔记（一）简介&amp;初探</title>
      <link href="/opengl-01/"/>
      <url>/opengl-01/</url>
      
        <content type="html"><![CDATA[<h1 id="OpenGL学习笔记（一）简介-amp-初探"><a href="#OpenGL学习笔记（一）简介-amp-初探" class="headerlink" title="OpenGL学习笔记（一）简介&amp;初探"></a>OpenGL学习笔记（一）简介&amp;初探</h1><h2 id="1-OpenGL"><a href="#1-OpenGL" class="headerlink" title="1. OpenGL"></a>1. OpenGL</h2><h3 id="OpenGL-简介"><a href="#OpenGL-简介" class="headerlink" title="OpenGL 简介"></a>OpenGL 简介</h3><ul><li><strong>OpenGL</strong>（Open Graphics Library，开放式图形库）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序接口（API），包含了一系列可以操作图形、图像的函数。<a href="https://baike.baidu.com/item/OpenGL/238984?fr=aladdin" title="OpenGL-百度百科" target="_blank" rel="noopener">[1]</a><a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li><li>OpenGL官网的描述其为”<em>The Industry’s Foundation for High Performance</em>“<a href="https://www.opengl.org" title="OpenGL 官网" target="_blank" rel="noopener">[3]</a>，OpenGL本身并不是一个API，仅仅是一个<strong>由Khronos组织制定并维护的规范</strong>（Specification）。OpenGL规范严格规定了每个函数如何运行，以及其输出输出值，而函数的具体实现，则由OpenGL库的开发者自行决定。<a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li><li>实际上，通常是<strong>显卡的生产商</strong>开发具体的OpenGL库，如使用Apple系统时，OpenGL库是由Apple自身维护。当OpenGL产生bug时，可以通过<strong>升级显卡驱动</strong>解决。<a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li></ul><h3 id="OpenGL-相关库"><a href="#OpenGL-相关库" class="headerlink" title="OpenGL 相关库"></a>OpenGL 相关库</h3><ul><li><strong>OpenGL core Library</strong><ul><li>OpenGL核心库</li></ul></li><li><strong>OpenGL Utility Library</strong>（GLU）</li><li><strong>OpenGL Utility Toolkit</strong>（GLUT）<ul><li>提供窗口系统的功能命令</li></ul></li><li><strong>freeglut</strong><ul><li>更新OpenGL的库</li></ul></li><li><strong>OpenGL Extension Wrangler Library</strong>（GLEW）</li></ul><h3 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h3><ul><li>1980s，软件开发人员为每种图形硬件编写自定义的借口和驱动程序。</li><li>1990初，SGI成为工作站3D图形领域的领导者，其IRIS GL API成为事实上的行业标准。其竞争对手（Sun、惠普、IBM）通过扩展PHIGS标准也将3D硬件投入市场，SCI 将 IRIS GL API转变为一项开放标准，即OpenGL。</li><li>1992年，SCI领导OpenGL架构审查委员会（OpenGL ARB）的创建。</li><li>1995年，微软发布Direct3D成为OpenGL的主要竞争对手。</li><li>1997年12月17日，微软和SGI发起华氏温标项目，统一OpenGL和Direct3D接口。1998年，惠普加入，但项目于1999年停止。</li><li>2006年7月，OpenGL架构评审委员会将OpenGL API标准的控制权交给Khronos Group。 </li></ul><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ul><li>此部分主要参考<a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" title="LearnOpenGL CN-入门" target="_blank" rel="noopener">[2]</a></li></ul><h4 id="核心模式-与-立即渲染模式"><a href="#核心模式-与-立即渲染模式" class="headerlink" title="核心模式 与 立即渲染模式"></a>核心模式 与 立即渲染模式</h4><ul><li><strong>立即渲染模式</strong>（Immediate mode, 固定渲染管线）：早期OpenGL使用，绘制图形容易使用和理解，但是效率较低。</li><li><strong>核心模式</strong>（Core-profile）：从OpenGL3.2后开始使用，移除了旧的特性，迫使用户使用现代的函数。</li></ul><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><ul><li><strong>扩展</strong>（Extension）：支持扩展，未成为规范之前，显卡公司提出的新特性或优化可以通过扩展的方式在驱动中实现。</li></ul><h4 id="状态机"><a href="#状态机" class="headerlink" title="状态机"></a>状态机</h4><ul><li>OpenGL自身是一个巨大的<strong>状态机</strong>（State Machine）：一系列的变量描述此时的OpenGL如何运行。OpenGL的状态通常被称为OpenGL上下文。</li><li>使用OpenGL时，使用<strong>状态设置函数</strong>（State-changing Function）与 <strong>状态使用函数</strong>（State-using Function）来进行操作。</li></ul><h2 id="2-初次尝试"><a href="#2-初次尝试" class="headerlink" title="2. 初次尝试"></a>2. 初次尝试</h2><ul><li>以下内容针对Mac上的Xcode（版本 Xcode11），其他平台的搭建还请参考网上的其他资源（MAC XCODE已自带GLUT.framework和OpenGL.framework，所以初次尝试不涉及“复杂”的环境配置）。以下内容参考<a href="https://www.cnblogs.com/chenyangsocool/p/5357691.html" title="Max Xcode 下配置OpenGL - chenyangsocool - 博客园" target="_blank" rel="noopener">[4]</a></li></ul><ol><li><p>打开Xcode，新建一个Command Line Tool项目。</p><p><img src="./02-01.png" alt="01-01 新建Command Line Tool项目"></p></li><li><p>添加GLUT.framework和OpenGL.framework</p><ol><li><p>点击Build Phases, Link Binary with Libraries</p><p><img src="./02-02.png" alt="01-02 添加库位置"></p></li><li><p>寻找、添加GLUT.framework和OpenGL.framework</p><p><img src="./02-03.png" alt="01-03 添加framework"></p></li><li><p>完成后效果</p><p><img src="./02-04.png" alt="01-04 添加后效果"></p></li></ol></li><li><p>进入main.cpp编写Demo代码并运行</p><ol><li><p>Demo代码</p><pre class=" language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">//绘制一个正方形</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;GLUT/GLUT.h></span></span><span class="token keyword">void</span> <span class="token function">myDisplay</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">glClear</span><span class="token punctuation">(</span>GL_COLOR_BUFFER_BIT<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glRectf</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5f</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5f</span><span class="token punctuation">,</span> <span class="token number">0.5f</span><span class="token punctuation">,</span> <span class="token number">0.5f</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glFlush</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">glutInit</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>argc<span class="token punctuation">,</span> argv<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitDisplayMode</span><span class="token punctuation">(</span>GLUT_RGB <span class="token operator">|</span> GLUT_SINGLE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitWindowPosition</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutInitWindowSize</span><span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutCreateWindow</span><span class="token punctuation">(</span><span class="token string">"第一个 OpenGL 程序"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutDisplayFunc</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>myDisplay<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">glutMainLoop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li><p>运行效果</p><img src="./02-05.png" alt="01-05" style="zoom:30%;"></li><li><p>由于在macOS 10.9、10.14 后一些命令被废止，会出现以下提示信息，下一篇会使用GLEW库。</p><p><img src="./02-06.png" alt="01-06 XCODE中的提示"></p></li></ol></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]: <a href="https://baike.baidu.com/item/OpenGL/238984?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/OpenGL/238984?fr=aladdin</a>    “OpenGL-百度百科”<br>[2]: <a href="https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/" target="_blank" rel="noopener">https://learnopengl-cn.github.io/01%20Getting%20started/01%20OpenGL/</a>    “LearnOpenGL CN-入门”<br>[3]: <a href="https://www.opengl.org" target="_blank" rel="noopener">https://www.opengl.org</a>    “OpenGL 官网”<br>[4]: <a href="https://www.cnblogs.com/chenyangsocool/p/5357691.html" target="_blank" rel="noopener">https://www.cnblogs.com/chenyangsocool/p/5357691.html</a>    “Max Xcode 下配置OpenGL - chenyangsocool - 博客园”</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读笔记&lt;03&gt;</title>
      <link href="/article-2019-03/"/>
      <url>/article-2019-03/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-论文阅读笔记-lt-03-gt"><a href="#2019-论文阅读笔记-lt-03-gt" class="headerlink" title="2019 论文阅读笔记 &lt;03&gt;"></a>2019 论文阅读笔记 &lt;03&gt;</h1><h2 id="【基本信息】"><a href="#【基本信息】" class="headerlink" title="【基本信息】"></a>【基本信息】</h2><ul><li><strong>论文题目：</strong><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" target="_blank" rel="noopener">Attention is all you need</a></li><li><strong>作者：</strong>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Adian N. Gomez, Łukasz Kaiser, Illia Polosukhin </li><li><strong>出版：</strong> <a href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017" target="_blank" rel="noopener">Advances in Neural Information Processing Systems 30 (NIPS 2017)</a></li><li><strong>日期：</strong>Submitted on 12 Jun 2017 （*<a href="https://arxiv.org/abs/1706.03762v1" target="_blank" rel="noopener">v1</a>*）, last revised 6 Dec 2017 （this version, v5）</li><li><strong>简介：</strong>此论文针对以往NLP中使用RNN结构和endoder-decoder结构无法并行、速度慢的问题，基于<strong>注意力机制</strong>提出了一种称为<strong>Transformer</strong>的网络结构。</li></ul><h2 id="【词汇术语】"><a href="#【词汇术语】" class="headerlink" title="【词汇术语】"></a>【词汇术语】</h2><h3 id="专业词汇-术语"><a href="#专业词汇-术语" class="headerlink" title="专业词汇/术语"></a>专业词汇/术语</h3><ul><li><strong>sequence transduction models</strong></li><li><strong>attention mechanism:</strong> 注意力机制</li></ul><h3 id="语言词汇"><a href="#语言词汇" class="headerlink" title="语言词汇"></a>语言词汇</h3><ul><li><strong>push the boundaries</strong></li><li><strong>integral</strong> 必需的</li></ul><h3 id="模型-模式-算法"><a href="#模型-模式-算法" class="headerlink" title="模型/模式/算法"></a>模型/模式/算法</h3><ul><li><strong>softmax</strong><ul><li>softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类</li><li>$S_i=\frac{e^i}{\sum_je^j}$</li></ul></li></ul><h2 id="【论文笔记】"><a href="#【论文笔记】" class="headerlink" title="【论文笔记】"></a>【论文笔记】</h2><h3 id="「简介前言」"><a href="#「简介前言」" class="headerlink" title="「简介前言」"></a>「简介前言」</h3><h4 id="当前缺陷"><a href="#当前缺陷" class="headerlink" title="当前缺陷"></a>当前缺陷</h4><ul><li><strong>RNN</strong><ul><li>前后状态的依赖性导致无法并行，在较长的词序时运行缓慢</li></ul></li><li><strong>当前努力</strong><ul><li>分解（factorization tricks）, 条件计算（conditional computation）</li><li><em>Neural GPU<em>，</em>ByteNet</em> 和 <em>ConvS2S</em></li><li>按序计算的限制仍然存在</li></ul></li></ul><h4 id="注意力机制（Attention-Mechanisms）"><a href="#注意力机制（Attention-Mechanisms）" class="headerlink" title="注意力机制（Attention Mechanisms）"></a>注意力机制（Attention Mechanisms）</h4><ul><li><strong>注意力机制</strong>（Attention Mechanisms）<ul><li>各种序列模型建模和转换模型中的一个组成部分（<em>an integral part of compelling sequence modeling and transduction models in various tasks</em>）</li><li>可以不用考虑在输入输出序列中的距离而允许建模</li><li>在少量情况，和循环网络结合使用</li></ul></li><li><strong>自注意力机制</strong>（Self-attention，intra-attention）<ul><li>把输入序列上不同位置的信息联系起来计算一个句子的表示内容</li></ul></li></ul><h3 id="「Transformer模型」"><a href="#「Transformer模型」" class="headerlink" title="「Transformer模型」"></a>「Transformer模型」</h3><h4 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h4><ul><li><p><strong>结构图</strong></p><p><img src="./03-01the-transformer-model-architecture.png" alt="03-01the-transformer-model-architecture"></p></li><li><p><strong>Encoder</strong></p><ul><li>由6个基本层堆叠</li><li>每层含有两个子层：<ul><li>注意力机制（<em>multi-head self-attention mechanism</em>）</li><li>全连接前向网络（<em>simple, position-wise fully connected feed-forward network</em>）</li></ul></li><li>在两个子层中都有剩余连接（<em>residual connection</em>）和层标准化（<em>layer normalization</em>）<ul><li>子层的输出：$LayerNorm(x+Sublayer(x))$</li></ul></li></ul></li><li><p><strong>Decoder</strong></p><ul><li>在encoder的输出外再增加了一层注意力机制</li><li>掩饰（<em>masking</em>）<ul><li>确保每个位置的预测只基于前面已知位置的词</li></ul></li></ul></li></ul><h4 id="注意力机制（Attention）"><a href="#注意力机制（Attention）" class="headerlink" title="注意力机制（Attention）"></a>注意力机制（Attention）</h4><ul><li><p><strong>注意力机制</strong></p><ul><li>将一个查找和一组键值对映射到正确的输出（<em>mapping a query and a set of key-value pairs to an output</em>）</li></ul><p><img src="./03-02tow-attention.png" alt="03-02tow-attention"></p></li><li><p><strong>Scaled Dot-Product Attention</strong></p><ul><li>$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</li><li>$d_k$：keys的维度；$d_v$：value的维度</li><li>$Q$：查询矩阵；$K$：keys的矩阵；$V$：values的矩阵</li></ul></li><li><p><strong>Multi-Head Attention</strong></p><ul><li>$MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^{\bigcirc}$</li><li>$head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$</li></ul></li><li><p>文章使用的注意力机制</p><ul><li>Multi-Head Attention，三个方面：</li><li>Encoder-Decoder Attention层<ul><li>查询来自前一个decoder层，允许decoder的每个位置都能关注输入的所有位置</li></ul></li><li>Encoder层中的Self-attention层<ul><li>在self-attention层中所有的key、value和query都来自前一层的encoder，encoder的每个位置都能关注前一层输出的所有位置。</li></ul></li><li>Decoder层中的Self-attention层</li></ul></li></ul><h4 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h4><ul><li>每一层都有一个<strong>fukky connected feed-forward network</strong><ul><li>$FNN(x)=\max(0,xW_1+b_1)W_2+b_2$</li><li>两个线性转换，中间一个ReLU</li></ul></li></ul><h4 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h4><h4 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h4><ul><li>不使用递归结构和卷积结构，引入位置编码来使用输入顺序信息</li><li><strong>位置编码：</strong><ul><li>$PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})$</li><li>$PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})$</li><li>$pos$: position; $i$: dimension</li></ul></li><li>使用sin版本，因为它允许序列用到更长</li></ul><h3 id="「为什么使用自注意力机制」"><a href="#「为什么使用自注意力机制」" class="headerlink" title="「为什么使用自注意力机制」"></a>「为什么使用自注意力机制」</h3><ul><li>三个方面比较其与递归结构、卷积结构<ul><li>每层的计算复杂度（<em>total computational complexity per layer</em>）</li><li>能被并行计算的数量（<em>the amount of computation that can be parallelized</em>）</li><li>网络中长范围以来的路径长度（<em>the path length between long-range dependencies in the network</em>）</li></ul></li></ul><h3 id="「实验与结论」"><a href="#「实验与结论」" class="headerlink" title="「实验与结论」"></a>「实验与结论」</h3><ul><li>同前文，不加以赘述，有兴趣可以自行查看论文以及相关代码。</li></ul><h2 id="「小记总结」"><a href="#「小记总结」" class="headerlink" title="「小记总结」"></a>「小记总结」</h2><ul><li>论文中为提高并行能力，引入注意力机制代替递归结构和卷积结构，用位置编码来体现句子顺序信息。换种思路实现位置表示，NICE～是否还有其他方法，或者还有其他的耗时处理可以用更为简洁的方法来代替处理呢？</li></ul>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读笔记&lt;02&gt;</title>
      <link href="/article-2019-02/"/>
      <url>/article-2019-02/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-论文阅读笔记-lt-02-gt"><a href="#2019-论文阅读笔记-lt-02-gt" class="headerlink" title="2019 论文阅读笔记 &lt;02&gt;"></a>2019 论文阅读笔记 &lt;02&gt;</h1><h2 id="【基本信息】"><a href="#【基本信息】" class="headerlink" title="【基本信息】"></a>【基本信息】</h2><ul><li><strong>论文题目：</strong><a href="https://www.aclweb.org/anthology/D14-1181/" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></li><li><strong>作者：</strong>Yoon Kim</li><li><strong>组织：</strong>New York University</li><li><strong>出版：</strong>Association for Computational Linguistics</li><li><strong>关联：</strong>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</li><li><strong>日期：</strong>October 2014</li><li><strong>简介：</strong>此论文在CNN上使用词向量处理自然语言的问题，将词向量与深度学习（NLP与卷积神经网络）结合，并构建了一个简单的CNN模型进行实验，指出了词向量是神经网络对于自然语言处理的一个重要部分<em>（unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP）</em>。</li></ul><h2 id="【词汇术语】"><a href="#【词汇术语】" class="headerlink" title="【词汇术语】"></a>【词汇术语】</h2><h3 id="专业词汇-术语"><a href="#专业词汇-术语" class="headerlink" title="专业词汇/术语"></a>专业词汇/术语</h3><ul><li><strong>tanh</strong>: hyperbolic tangent, 双曲正切函数</li><li><strong>pooling:</strong> 池化层，集中特征</li><li><strong>Regularization:</strong> 正则化<ul><li>给损失函数加一个正则化项</li><li><strong>l2-norms</strong></li></ul></li></ul><h3 id="语言词汇"><a href="#语言词汇" class="headerlink" title="语言词汇"></a>语言词汇</h3><ul><li><strong>utilize:</strong> 利用</li><li><strong>disentangle:</strong> 摆脱</li></ul><h3 id="模型-模式-算法"><a href="#模型-模式-算法" class="headerlink" title="模型/模式/算法"></a>模型/模式/算法</h3><ul><li>$\circ$: the element-wise multiplication operator<ul><li>数组对应元素相乘</li></ul></li><li><strong>Bernoulli random variables</strong>：伯努利随机变量<ul><li>01分布：$P(X=1)=p,\ P(x=1)=1-p$</li></ul></li><li><strong>norm:</strong> 范数<ul><li>$l_2-norms(||w||_2)$: 第二范数，欧几里得范数<ul><li>$||w||_2=\sqrt{x_1^2+x_2^2+…+x_n^2}$</li></ul></li></ul></li></ul><h2 id="【论文笔记】"><a href="#【论文笔记】" class="headerlink" title="【论文笔记】"></a>【论文笔记】</h2><h3 id="「简介内容」"><a href="#「简介内容」" class="headerlink" title="「简介内容」"></a>「简介内容」</h3><ul><li><strong>深度学习</strong>（Deep Learning）：当时主要应用于计算机视觉、语音识别，在自然语言处理方面，则用于特征分类上。</li><li><strong>词向量</strong>（Word Vectors）</li><li><strong>卷积神经网络</strong>（Convolutional neural networks, CNN）<ul><li>利用卷积过滤器层</li><li>最早用于计算机视觉</li><li>在语义分析上展现出能力</li></ul></li></ul><h3 id="「模型」"><a href="#「模型」" class="headerlink" title="「模型」"></a>「模型」</h3><h4 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图    "></a>模型结构图    <img src="./02-01cnn-model-architecture.png" alt="CNN Model Architecture"></h4><h4 id="变量信息"><a href="#变量信息" class="headerlink" title="变量信息"></a>变量信息</h4><ul><li>$x_i$: $k$维词向量句子中的第$i$个词语</li><li>$x_{1:n}$: 长度为n的句子</li><li>$w$: 长度为$h$的窗口</li><li>特征: $c_i=f(w\cdot x_{i:i+h-1}+b)$<ul><li>$f$: 非线性函数</li><li>$b$: 偏移</li></ul></li><li>$\hat{c}=\max{c}$</li></ul><h4 id="各层信息"><a href="#各层信息" class="headerlink" title="各层信息"></a>各层信息</h4><ul><li><strong>池化层</strong>：max-pooling方法</li><li><strong>全连接层</strong>：softmax</li></ul><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul><li>正则化用于防止过拟合，常见是在损失函数中加入正则项（L1、L2正则化），本文采用的是DropOut方法。</li><li><strong>Dropout</strong><ul><li>训练中随机删掉隐藏层的一半节点进行学习，之后再对另一半节点进行一轮学习，最终将权重除以2。</li><li>减少了神经元间的依赖性，降低了过拟合，提高了准确率。</li></ul></li></ul><h3 id="「实验结论」"><a href="#「实验结论」" class="headerlink" title="「实验结论」"></a>「实验结论」</h3><ul><li>实验方法、结果不再重复，有兴趣可以自行阅读论文。</li><li>最重要的一点是，该论文指出词向量是神经网络对于自然语言处理的一个重要部分（<em>Our results add to the well-established evidence that unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP.</em>）。</li></ul><h2 id="【小记总结】"><a href="#【小记总结】" class="headerlink" title="【小记总结】"></a>【小记总结】</h2><ul><li>此论文作为CNN与NLP结合的开山之作，虽然模型简单，但却极具价值和意义。创造性、拓展性的思维和能力，急需。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读笔记&lt;01&gt;</title>
      <link href="/article-2019-01/"/>
      <url>/article-2019-01/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-论文阅读笔记-lt-01-gt"><a href="#2019-论文阅读笔记-lt-01-gt" class="headerlink" title="2019 论文阅读笔记 &lt;01&gt;"></a>2019 论文阅读笔记 &lt;01&gt;</h1><h2 id="【基本信息】"><a href="#【基本信息】" class="headerlink" title="【基本信息】"></a>【基本信息】</h2><ul><li><strong>论文题目：</strong> <em><a href="https://ieeexplore.ieee.org/document/7372153" target="_blank" rel="noopener">Entropy-Based Term Weighting Schemes for Text Categorization in VSM</a></em></li><li><strong>作者：</strong>Tao Wang ; Yi Cai ; Ho-fung Leung ; Zhiwei Cai ; Huaqing Min</li><li><strong>出版：</strong> <a href="https://ieeexplore.ieee.org/xpl/conhome/7372093/proceeding" target="_blank" rel="noopener">2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)</a></li><li><strong>日期：</strong>9-11 Nov. 2015 / 07 Jan 2016</li><li><strong>标签：</strong>TC、VSM</li><li><strong>简介：</strong>此论文分析了用于文本分类任务的一些权重法（如无监督学习中的$tf,tf\cdot idf,BM25$ 与监督学习中的$rf,iqf\cdot qf\cdot icf,tf\cdot gr$等），指出这些已有模式在文本分类中存在的问题，并提出了两种新的基于熵的权重法（$tf\cdot dc$ 和 $tf\cdot bdc$），提升了术语的辨别力与文本分类任务的完成。</li></ul><h2 id="【词汇术语】"><a href="#【词汇术语】" class="headerlink" title="【词汇术语】"></a>【词汇术语】</h2><h3 id="专业词汇-术语"><a href="#专业词汇-术语" class="headerlink" title="专业词汇/术语"></a>专业词汇/术语</h3><ul><li><strong>TC</strong> （Text Categorization）：文本分类</li><li><strong>VSM</strong> （Vector Space Model）：向量空间模型</li><li><strong>IR</strong> （Information Retrieval）：信息检索</li><li><strong>unsupervised</strong> &amp; <strong>supervised</strong>：无监督、监督</li><li><strong>Contingency Table</strong> 情形分析表<ul><li><strong>positive category （PC）</strong> ：正类</li><li><strong>negative category （NC）</strong>：负类</li></ul></li></ul><h3 id="语言词汇"><a href="#语言词汇" class="headerlink" title="语言词汇"></a>语言词汇</h3><ul><li><strong>state-of-the-art</strong>：当前最好的</li></ul><h3 id="模型-模式-算法"><a href="#模型-模式-算法" class="headerlink" title="模型/模式/算法"></a>模型/模式/算法</h3><ul><li><strong>KNN</strong> （k-Nearest Neighbor）：邻近算法</li><li><strong>SVM</strong> （Support Vector Machine）：支持向量机</li><li>$tf$：<strong>Term Frequency</strong> 词频<ul><li>$词频(TF)=某个词在文章中的出现次数$</li><li>$词频(TF)=\frac{某个词在文章中的出现次数}{文章的总词数}$</li><li>$词频(TF)=\frac{某个词在文章中的出现次数}{该文出现次数最多的词的出现次数}$</li><li>Variants: $\log(tf),\log(tf+1),log(tf)+1,…$</li></ul></li><li>$idf$: <strong>Inverse document frequency</strong> 逆文本频率指数<ul><li>$逆文档频率(IDF)=\log(\frac{语料库的文档总数}{包含该词的文档数+1})$</li><li>$df$: <strong>document frequency</strong></li><li>包含词条的文档越小，$idf$越大</li></ul></li><li>$dc$: <strong>distributional concentration</strong><ul><li>$dc(t)=1-\frac{H(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{f(t,c_i)}{f(t)}\log{\frac{f(t,c_i)}{f(t)}}}{\log{|C|}}$</li></ul></li><li>$rf$: <strong>relevance frequency</strong>, $rf=\frac{a}{c}$</li><li>$cf$: <strong>category frequency</strong>, 出现的类别越少，值越大</li><li>$bdc$: <strong>balanced distributional concentration</strong><ul><li>$bdc(t)=1-\frac{BH(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{p(t|c_i)}{\sum^{|C|}_{i=1}p(t|c_i)}\log{\frac{p(t|c_i)}{sum^{|C|}_{i=1}p(t|c_i)}}}{\log{|C|}}$</li></ul></li></ul><h2 id="【论文笔记】"><a href="#【论文笔记】" class="headerlink" title="【论文笔记】"></a>【论文笔记】</h2><h3 id="「已有方法-—-无监督学习方法」"><a href="#「已有方法-—-无监督学习方法」" class="headerlink" title="「已有方法 —- 无监督学习方法」"></a>「已有方法 —- 无监督学习方法」</h3><h4 id="无监督学习（unsupervised）"><a href="#无监督学习（unsupervised）" class="headerlink" title="无监督学习（unsupervised）"></a>无监督学习（unsupervised）</h4><ul><li><strong>含义：</strong>无先验知识（无标签）的学习</li><li><strong>常见模式：</strong>$tf,tf\cdot idf,BM25 $</li></ul><h4 id="缺点问题"><a href="#缺点问题" class="headerlink" title="缺点问题"></a>缺点问题</h4><ul><li><strong>【由于】</strong>关注于词出现的次数，而忽略了训练<em>文档（documents）<em>的</em>类别标签（category labels）</em></li><li><strong>【导致】</strong><ul><li><em>词语（term）</em>能区别文档的差异，但不能区别类别的差异</li><li>在文本分类任务中，不足以衡量词语对文档类别的<em>辨别能力（discriminating power）</em></li></ul></li></ul><h3 id="「已有方法-—-监督学习方法」"><a href="#「已有方法-—-监督学习方法」" class="headerlink" title="「已有方法 —- 监督学习方法」"></a>「已有方法 —- 监督学习方法」</h3><h4 id="监督学习（supervised）"><a href="#监督学习（supervised）" class="headerlink" title="监督学习（supervised）"></a>监督学习（supervised）</h4><ul><li><strong>含义：</strong>利用已知类别的样本进行学习</li><li><strong>常见模式：</strong>$rf,iqf\cdot qf\cdot icf,tf\cdot gr$</li></ul><h4 id="缺点问题-1"><a href="#缺点问题-1" class="headerlink" title="缺点问题"></a>缺点问题</h4><ul><li><strong>缘由：</strong>大部分监督学习用到了情形分析表中的正类（PC）和负类（NC）</li><li><strong>【由于】</strong>在<em>多类别的情况（multi-class case）</em>中，正类只有一个类，而负类是多个类的集合</li><li><strong>【导致】</strong>负类中产生了<em>信息损失（information loss）</em></li></ul><h4 id="已有优化仍然存在的问题"><a href="#已有优化仍然存在的问题" class="headerlink" title="已有优化仍然存在的问题"></a>已有优化仍然存在的问题</h4><ul><li><em>正类和负类的分离问题 （PC/NC-split based schemes）</em>仍然存在，使得无法有效区别类别。</li><li><em>一种基于统计置信区间的模式（a scheme based on statistical confidence intervals）</em><ul><li>过于复杂难以实现</li></ul></li></ul><h3 id="「基于熵的权重法」"><a href="#「基于熵的权重法」" class="headerlink" title="「基于熵的权重法」"></a>「基于熵的权重法」</h3><h4 id="基于熵的权重法（entropy-based-term-weighting-schemes）"><a href="#基于熵的权重法（entropy-based-term-weighting-schemes）" class="headerlink" title="基于熵的权重法（entropy- based term weighting schemes）"></a>基于熵的权重法（entropy- based term weighting schemes）</h4><ul><li><strong>观点</strong><ul><li>利用负类中的具体类别帮助提升辨别力</li><li><em>更高浓度（higher concentration）</em>的词语具有更强的辨别能力</li><li>在类别层面有更高浓度分布的词语的熵更小</li></ul></li><li>熵值越小，辨别能力越强</li></ul><h4 id="模式1-Distribution-Concentration"><a href="#模式1-Distribution-Concentration" class="headerlink" title="模式1: Distribution Concentration"></a>模式1: Distribution Concentration</h4><ul><li><strong>模式</strong>：$dc$, distribution concentration</li><li><strong>计算公式：</strong> $dc(t)=1-\frac{H(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{f(t,c_i)}{f(t)}\log{\frac{f(t,c_i)}{f(t)}}}{\log{|C|}}$<ul><li>$|C|$: 类别数量</li><li>$f(t,c_i)$: 表示词语 $t$ 在类别 $c_i$ 中出现的频率，这里使用 $df(t,c_i)$</li><li>$f(t)$: 词语 $t$ 在所有类别出现的频数和</li><li>$H(t)$: 词语 $t$ 在语料库中类别的熵，$H(t)\in [0,\log|C|]$</li></ul></li><li><strong>特征优势</strong><ul><li>词语的权重基于在类别中词语的全局分布，而不是依赖于已有的类别正类。</li><li>因此，不需要已有的正类标签就可以进行分类。</li></ul></li><li><strong>缺陷</strong><ul><li>缺少类别的优先级信息（不同种类文档的长度不同，会导致熵的计算产生偏差）<ul><li>$bdc$模式解决此问题</li></ul></li></ul></li></ul><h4 id="模式2-Balanced-Distributional-Concentration"><a href="#模式2-Balanced-Distributional-Concentration" class="headerlink" title="模式2: Balanced Distributional Concentration"></a>模式2: Balanced Distributional Concentration</h4><ul><li><strong>模式</strong>：$bdc$, balanced distributional concentration</li><li><strong>计算公式：</strong> $bdc(t)=1-\frac{BH(t)}{\log(|C|)}=1+\frac{\sum^{|C|}_{i=1}\frac{p(t|c_i)}{\sum^{|C|}_{i=1}p(t|c_i)}\log{\frac{p(t|c_i)}{sum^{|C|}_{i=1}p(t|c_i)}}}{\log{|C|}}$</li><li><strong>特征优势</strong><ul><li>解决了 $dc$ 模式存在的问题</li></ul></li></ul><h3 id="「实验结果」"><a href="#「实验结果」" class="headerlink" title="「实验结果」"></a>「实验结果」</h3><ul><li>此处对于实验的数据、内容、结论不做详细描述，有兴趣可以自行查看论文。</li></ul><h2 id="【小记总结】"><a href="#【小记总结】" class="headerlink" title="【小记总结】"></a>【小记总结】</h2><ul><li>2019论文博客整理的第一篇，希望能坚持，不足之处还请谅解。许多的英文词汇转为中文总有些怪异的感觉，就保留在中文词后面了。</li><li>此论文指出了过往文本分类任务中一些模式（scheme, 这个词翻译成模式也觉得读起来不太顺口，汗）的缺陷，无法很好地利用标签信息、区别类别差异，因而引入了熵的概念。一种角度来说，也是更充分地利用“压榨”已有信息（无监督学习模式忽略了类别标签，PC/NC的监督学习模式则忽略了NC中的类别差异），挖掘、压榨、充分利用全部信息，促进更好的分类。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019论文阅读列表</title>
      <link href="/article-2019-00/"/>
      <url>/article-2019-00/</url>
      
        <content type="html"><![CDATA[<h1 id="论文阅读列表-2019"><a href="#论文阅读列表-2019" class="headerlink" title="论文阅读列表-2019"></a>论文阅读列表-2019</h1><h2 id="在读-已读列表"><a href="#在读-已读列表" class="headerlink" title="在读/已读列表"></a>在读/已读列表</h2><ol><li><a href="https://ieeexplore.ieee.org/document/7372153" target="_blank" rel="noopener">Entropy-Based Term Weighting Schemes for Text Categorization in VSM</a></li><li><a href="https://www.aclweb.org/anthology/D14-1181/" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></li><li><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" target="_blank" rel="noopener">Attention is All you Need</a></li><li><a href="http://kns.cnki.net/kcms/detail/61.1167.g3.20190910.1730.010.html" target="_blank" rel="noopener">黄炜,黄建桥,李岳峰.基于BiLSTM-CRF的涉恐信息实体识别模型研究</a></li><li>Neural Architectures for Named Entity Recognition</li><li>文本自动生成研究进展与趋势</li></ol><h2 id="候选列表"><a href="#候选列表" class="headerlink" title="候选列表"></a>候选列表</h2><ol><li>Man Lan, Chew Lim Tan, Jian Su, and Yue Lu. Supervised and traditional term weighting methods for automatic text categorization. <em>Pattern Analysis and Machine Intelligence, IEEE Transactions on</em>, 31(4):721–735, 2009.</li><li>Xinghua Lu, Bin Zheng, Atulya Velivelli, and ChengXiang Zhai. Enhancing text categorization with semantic-enriched representation and training data augmentation. <em>Journal of the American Medical Informatics Association</em>, 13(5):526–535, 2006.</li></ol>]]></content>
      
      
      <categories>
          
          <category> Article </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Article </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
